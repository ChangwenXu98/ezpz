{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TEST `ezpz` 🍋\n",
        "\n",
        "\\| \\[Sam Foreman\n",
        "[<span class=\"orcid-green\"></span>](https://orcid.org/0000-0002-9981-0876)\\]()  \n",
        "2024-04-22\n",
        "\n",
        "## Overview\n",
        "\n",
        "> **<code>ezpz</code> 🍋**\n",
        ">\n",
        "> Launch and train across all your accelerators, using your favorite\n",
        "> framework + backend combo.\n",
        ">\n",
        "> `ezpz` simplifies the process of:\n",
        ">\n",
        "> -   <details>\n",
        ">\n",
        ">     <summary>\n",
        ">\n",
        ">     Setting up + launching distributed training:\n",
        ">     </summary>\n",
        ">\n",
        ">     -   <details closed>\n",
        ">\n",
        ">         <summary>\n",
        ">\n",
        ">         <code>import ezpz as ez</code>\n",
        ">         </summary>\n",
        ">\n",
        ">         -   `RANK =`\n",
        ">             [`ez.setup_torch(backend=backend)`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#L551)\n",
        ">             <span class=\"dim-text\">for `backend` $\\in$ {`DDP`,\n",
        ">             `deepspeed`, `horovod`}</span>\n",
        ">\n",
        ">         -   `RANK =`\n",
        ">             [`ez.get_rank()`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#396)\n",
        ">\n",
        ">         -   `LOCAL_RANK =`\n",
        ">             [`ez.get_local_rank()`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#448)\n",
        ">\n",
        ">         -   `WORLD_SIZE =`\n",
        ">             [`ez.get_world_size()`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#L417)\n",
        ">\n",
        ">         <span class=\"dim-text\">(see\n",
        ">         [`ezpz/dist.py`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py)\n",
        ">         for more details).</span>\n",
        ">\n",
        ">     </details>\n",
        ">\n",
        "> </details>\n",
        ">\n",
        "> -   <details closed>\n",
        ">\n",
        ">     <summary>\n",
        ">\n",
        ">     Using your favorite <code>{framework, backend}</code>\n",
        ">\n",
        ">     </summary>\n",
        ">\n",
        ">     On any accelerator:\n",
        ">\n",
        ">     -   [`framework=pytorch`](#pytorch) +\n",
        ">         `backend={DDP, deepspeed, horovod}`\n",
        ">\n",
        ">     -   [`framework=tensorflow`](#tensorflow) + `backend=horovod`\n",
        ">\n",
        ">     -   [`ez.get_torch_device()`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#L332):\n",
        ">         {`cuda`, `xpu`, `mps`, `cpu`}\n",
        ">\n",
        ">     -   [`ez.get_torch_backend()`](https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#L348):\n",
        ">         {`nccl`, `ccl`, `gloo`}\n",
        ">\n",
        ">     *2ez* 😎. (see [frameworks](#frameworks) for additional details)\n",
        ">\n",
        "> </details>\n",
        ">\n",
        "> -   <details closed>\n",
        ">\n",
        ">     <summary>\n",
        ">\n",
        ">     Writing device agnostic code:\n",
        ">     </summary>\n",
        ">\n",
        ">     -   <details>\n",
        ">\n",
        ">         <summary>\n",
        ">\n",
        ">         <a href=\"https://github.com/saforem2/ezpz/blob/main/src/ezpz/dist.py#L332\"><code>ezpz.get_torch_device()</code></a>\n",
        ">         </summary>\n",
        ">\n",
        ">         ``` python\n",
        ">         >>> import ezpz as ez\n",
        ">         >>> DEVICE = ez.get_torch_device()\n",
        ">         >>> model = torch.nn.Linear(10, 10)\n",
        ">         >>> model.to(DEVICE)\n",
        ">         >>> x = torch.randn((10, 10), device=DEVICE)\n",
        ">         >>> y = model(x)\n",
        ">         >>> y.device\n",
        ">         device(type='mps', index=0)\n",
        ">         ```\n",
        ">\n",
        ">     </details>\n",
        ">\n",
        "> -   <details closed>\n",
        ">\n",
        ">     <summary>\n",
        ">\n",
        ">     Using <code>wandb</code>:\n",
        ">     </summary>\n",
        ">\n",
        ">     -   `ez.setup_wandb(project_name='ezpz')`\n",
        ">\n",
        ">     </details>\n",
        ">\n",
        "> -   **Full support** for any {`device` + `framework` + `backend`}:\n",
        ">     -   device: {`GPU`, `XPU`, `MPS`, `CPU`}\n",
        ">     -   framework: {`torch`, `deepspeed`, `horovod`, `tensorflow`}\n",
        ">     -   backend: {`DDP`, `deepspeed`, `horovod`}\n",
        "\n",
        "## 📝 Example\n",
        "\n",
        "We provide below a complete example that will launch\n",
        "[`test_dist.py`](./src/ezpz/test_dist.py) across all GPUs in your\n",
        "current {`PBS`, `slurm`} job and train a simple model using either `DDP`\n",
        "or `deepspeed`.\n",
        "\n",
        "1.  `git clone` + `pip install ezpz`:\n",
        "\n",
        "    ``` bash\n",
        "    $ git clone https://github.com/saforem2/ezpz\n",
        "    $ python3 -m pip install -e ezpz\n",
        "    ```\n",
        "\n",
        "2.  <span class=\"dim-text\">\\[optional\\]</span> If using `PBS` or\n",
        "    `slurm`:\n",
        "\n",
        "    -   <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        Using a Job Scheduler:\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        Save Job info [`savejobenv`](./src/ezpz/bin/savejobenv):\n",
        "\n",
        "        ``` bash\n",
        "        $ source ezpz/src/ezpz/bin/savejobenv\n",
        "        ```\n",
        "\n",
        "        <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        <code>output</code>:\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        ``` bash\n",
        "        ┌───────────────────────────────────────────────────────────────────\n",
        "        │ Writing PBS vars to /home/foremans/.pbsenv\n",
        "        │ HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\n",
        "        │ NHOSTS: 2\n",
        "        │ NGPU_PER_HOST: 12 GPUs per host\n",
        "        │ NGPUS: 24 GPUs total\n",
        "        └───────────────────────────────────────────────────────────────────\n",
        "        ┌───────────────────────────────────────────────────────────────────\n",
        "        │ [DIST INFO]:\n",
        "        │   • Writing Job info to /home/foremans/.pbsenv\n",
        "        │     • HOSTFILE: /var/spool/pbs/aux/8992614.amn-0001\n",
        "        │     • NHOSTS: 2\n",
        "        │     • NGPU_PER_HOST: 12\n",
        "        │     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 24\n",
        "        └──────────────────────────────────────────────────────────────────\n",
        "        ┌──────────────────────────────────────────────────────────────────\n",
        "        │ [Hosts]:\n",
        "        │       • x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com, x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\n",
        "        │     • [host:0] - x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\n",
        "        │     • [host:1] - x1921c0s2b0n0.hostmgmt2000.cm.americas.sgi.com\n",
        "        └──────────────────────────────────────────────────────────────────\n",
        "        ┌────────────────────────────────────────────────────────────────────────────────\n",
        "        │ YOU ARE HERE: /home/foremans\n",
        "        │ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n",
        "        └────────────────────────────────────────────────────────────────────────────────\n",
        "        ┌──────────────────────────────────────────────────────────────────\n",
        "        │ [Launch]:\n",
        "        │     • Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\n",
        "        │       to launch job\n",
        "        └───────────────────────────────────────────────────────────────────\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "        this will automatically define a `launch` alias:\n",
        "\n",
        "        ``` bash\n",
        "        ┌──────────────────────────────────────────────────────────────────\n",
        "        │ [Launch]:\n",
        "        │     • Use: 'launch' (=mpiexec --verbose --envall -n 24 -ppn 12 --hostfile /var/spool/pbs/aux/8992614.amn-0001)\n",
        "        │       to launch job\n",
        "        └───────────────────────────────────────────────────────────────────\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "3.  Launch [`test_dist.py`](./src/ezpz/test_dist.py):\n",
        "\n",
        "    -   <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        <a href=\"https://github.com/saforem2/ezpz/blob/main/src/ezpz/test_dist.py\"><code>test_dist.py</code></a>:\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        ``` python\n",
        "        \"\"\"\n",
        "        ezpz_ddp.py\n",
        "\n",
        "        - to launch:\n",
        "\n",
        "          $ source ezpz/src/ezpz/bin/savejobenv\n",
        "          $ BACKEND=DDP launch python3 ezpz_ddp.py\n",
        "        \"\"\"\n",
        "        import os\n",
        "        import logging\n",
        "        import time\n",
        "        from typing import Optional\n",
        "        import torch\n",
        "        import ezpz as ez\n",
        "\n",
        "        # backend can be any of DDP, deespepeed, horovod\n",
        "        DIST_INIT = ez.setup_torch_distributed(\n",
        "            backend=(\n",
        "                backend := os.environ.get('BACKEND', 'DDP')\n",
        "            ),\n",
        "            port=(\n",
        "                port := os.environ.get(\"MASTER_PORT\", \"29500\")\n",
        "            )\n",
        "        )\n",
        "        DEVICE = ez.get_torch_device()\n",
        "        RANK = DIST_INIT['rank']\n",
        "        WORLD_SIZE = DIST_INIT['world_size']\n",
        "        LOCAL_RANK = DIST_INIT['local_rank']\n",
        "        DEVICE_ID = f\"{DEVICE}:{LOCAL_RANK}\"\n",
        "        _ = ez.print_dist_setup()\n",
        "\n",
        "        if DEVICE == \"cuda\" and torch.cuda.is_available():\n",
        "            torch.cuda.set_device(LOCAL_RANK)\n",
        "\n",
        "        # log only from RANK == 0\n",
        "        logger = logging.getLogger(__name__)\n",
        "        logger.setLevel(\"INFO\") if RANK == 0 else logger.setLevel(\"CRITICAL\")\n",
        "\n",
        "        BATCH_SIZE = 64\n",
        "        INPUT_SIZE = 128\n",
        "        OUTPUT_SIZE = 128\n",
        "        DTYPE = torch.get_default_dtype()\n",
        "\n",
        "        logger.info(f\"{DIST_INIT=}\")\n",
        "\n",
        "        class Network(torch.nn.Module):\n",
        "            def __init__(\n",
        "                    self,\n",
        "                    input_dim: int = 128,\n",
        "                    output_dim: int = 128,\n",
        "                    sizes: Optional[list[int]] = None,\n",
        "            ):\n",
        "                super(Network, self).__init__()\n",
        "                if sizes is None:\n",
        "                    self.layers = torch.nn.Linear(input_dim, output_dim)\n",
        "                elif len(sizes) > 0:\n",
        "                    layers = [torch.nn.Linear(input_dim, sizes[0])]\n",
        "                    for idx, size in enumerate(sizes[1:]):\n",
        "                        layers.append(\n",
        "                            torch.nn.Linear(sizes[idx], size)\n",
        "                        )\n",
        "                    layers.append(torch.nn.Linear(sizes[-1], output_dim))\n",
        "                    self.layers = torch.nn.Sequential(*layers)\n",
        "\n",
        "            def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "                return self.layers(x)\n",
        "\n",
        "\n",
        "        def calc_loss(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
        "            return (y - x).pow(2).sum()\n",
        "\n",
        "\n",
        "        def main():\n",
        "            model = Network(\n",
        "                input_dim=INPUT_SIZE,\n",
        "                output_dim=OUTPUT_SIZE,\n",
        "                sizes=[1024, 512, 256, 128]\n",
        "            )\n",
        "            model.to(DEVICE)\n",
        "            model.to(DEVICE_ID)\n",
        "            logger.info(f'{model=}')\n",
        "            optimizer = torch.optim.Adam(model.parameters())\n",
        "            if WORLD_SIZE > 1:\n",
        "                if backend.lower() == 'ddp':\n",
        "                    from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "                    model = DDP(\n",
        "                        model,\n",
        "                        device_ids=[]\n",
        "                    )\n",
        "                elif backend.lower() in ('ds', 'deepspeed'):\n",
        "                    import deepspeed\n",
        "                    import argparse\n",
        "                    parser = argparse.ArgumentParser(description='My training script.')\n",
        "                    parser.add_argument(\n",
        "                        '--local_rank',\n",
        "                        required=False,\n",
        "                        type=int,\n",
        "                        default=-1, \n",
        "                        help='local rank passed from distributed launcher',\n",
        "                    ),\n",
        "                    # Include DeepSpeed configuration arguments\n",
        "                    parser = deepspeed.add_config_arguments(parser)\n",
        "                    cmd_args = parser.parse_args()\n",
        "                    logger.info(f'{cmd_args=}')\n",
        "                    model, optimizer, *_ = deepspeed.initialize(\n",
        "                        args=cmd_args,\n",
        "                        model=model,\n",
        "                        optimizer=optimizer,\n",
        "                    )\n",
        "\n",
        "            for iter in range(10):\n",
        "                t0 = time.perf_counter()\n",
        "                x = torch.rand((BATCH_SIZE, INPUT_SIZE), dtype=DTYPE).to(DEVICE)\n",
        "                y = model(x)\n",
        "                loss = calc_loss(x, y)\n",
        "                dtf = ((t1 := time.perf_counter()) - t0)\n",
        "                if backend == 'deepspeed':\n",
        "                    model.backward(loss)\n",
        "                    model.step(loss)\n",
        "                else:\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                dtb = time.perf_counter() - t1\n",
        "                logger.info(\n",
        "                    ', '.join([\n",
        "                        f'{iter=}',\n",
        "                        f'loss={loss.item():.5f}',\n",
        "                        f'dt={dtf+dtb:.3f}',\n",
        "                        f'{dtf=:.3f}',\n",
        "                        f'{dtb=:.3f}'\n",
        "                    ])\n",
        "                )\n",
        "\n",
        "\n",
        "        if __name__ == '__main__':\n",
        "          main()\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "    -   DDP:\n",
        "\n",
        "        ``` bash\n",
        "        $ launch python3 -m ezpz.test_dist\n",
        "        ```\n",
        "\n",
        "    -   DeepSpeed:\n",
        "\n",
        "        ``` bash\n",
        "        $ BACKEND=deepspeed launch python3 -m ezpz.test_dist --deepspeed --deepspeed_config ezpz/src/ezpz/conf/ds_config.json\n",
        "        ```\n",
        "\n",
        "    -   <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        Output:\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        <code>GPU</code>\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        ``` bash\n",
        "        # [07:26:13 PM] [foremans@x3005c0s25b1n0] ~ 2024-04-20\n",
        "        $ launch python3 -m ezpz.test_dist |& tee ezpz-test-dist.log\n",
        "\n",
        "        Connected to tcp://x3005c0s13b0n0.hsn.cm.polaris.alcf.anl.gov:7919\n",
        "        Found executable /lus/eagle/projects/datascience/foremans/miniconda3/envs/2024-04-20/bin/python3\n",
        "        Launching application 9e4c8311-1729-4385-b1d2-d4cd6006ac1d\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=1/7][local_rank=1/3][node=1/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=5/7][local_rank=1/3][node=1/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=3/7][local_rank=3/3][node=1/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=7/7][local_rank=3/3][node=1/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=4/7][local_rank=0/3][node=0/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=6/7][local_rank=2/3][node=0/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=2/7][local_rank=2/3][node=0/1]\n",
        "        [2024-04-20 19:26:22][INFO][dist:290] - [device='cuda'][rank=0/7][local_rank=0/3][node=0/1]\n",
        "        [2024-04-20 19:26:22][WARNING][dist:296] - Using [8 / 8] available \"cuda\" devices !!\n",
        "        [2024-04-20 19:26:22][INFO][test_dist:46] - DIST_INIT={'world_size': 8, 'rank': 0, 'local_rank': 0}\n",
        "        [2024-04-20 19:26:24][INFO][test_dist:84] - model=Network(\n",
        "          (layers): Sequential(\n",
        "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
        "            (1): Linear(in_features=1024, out_features=512, bias=True)\n",
        "            (2): Linear(in_features=512, out_features=256, bias=True)\n",
        "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
        "            (4): Linear(in_features=128, out_features=128, bias=True)\n",
        "          )\n",
        "        )\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=0, loss=2789.99072, dt=0.664, dtf=0.659, dtb=0.005\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=1, loss=1961.33459, dt=0.002, dtf=0.001, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=2, loss=1450.47461, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=3, loss=1088.81958, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=4, loss=945.28839, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=5, loss=906.78857, dt=0.002, dtf=0.000, dtb=0.001\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=6, loss=789.18243, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=7, loss=751.63477, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=8, loss=735.62915, dt=0.002, dtf=0.000, dtb=0.002\n",
        "        [2024-04-20 19:26:28][INFO][test_dist:126] - iter=9, loss=732.12775, dt=0.002, dtf=0.000, dtb=0.001\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "        <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        <code>XPU</code>\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        ``` bash\n",
        "        # [04:50:57 PM] [foremans@x1921c0s0b0n0] ~/q/llm.devkit/Megatron-DeepSpeed/dep/ezpz/s/ezpz  main q4-drop 32s\n",
        "        $ launch python3 -Wignore test_dist.py\n",
        "        Connected to tcp://x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com:7919\n",
        "        Found executable /home/foremans/miniconda3/envs/q4-drop/bin/python3\n",
        "        Launching application 5bf3e9e8-89fb-412a-a49e-3c81601436b7\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=9/23][local_rank=9/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=14/23][local_rank=2/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=3/23][local_rank=3/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=17/23][local_rank=5/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=6/23][local_rank=6/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=13/23][local_rank=1/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=7/23][local_rank=7/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=19/23][local_rank=7/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=8/23][local_rank=8/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=21/23][local_rank=9/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=10/23][local_rank=10/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=22/23][local_rank=10/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=11/23][local_rank=11/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=23/23][local_rank=11/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=2/23][local_rank=2/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=20/23][local_rank=8/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=4/23][local_rank=4/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=15/23][local_rank=3/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=18/23][local_rank=6/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=12/23][local_rank=0/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=1/23][local_rank=1/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=16/23][local_rank=4/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=5/23][local_rank=5/11][node=1/1]\n",
        "        [2024-04-19 16:51:06][INFO][dist:239] - DistInfo={\n",
        "            \"DEVICE\": \"xpu\",\n",
        "            \"DEVICE_ID\": \"xpu:0\",\n",
        "            \"DISTRIBUTED_BACKEND\": \"ccl\",\n",
        "            \"GPUS_PER_NODE\": 12,\n",
        "            \"HOSTFILE\": \"/var/spool/pbs/aux/8992337.amn-0001\",\n",
        "            \"HOSTNAME\": \"x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\",\n",
        "            \"HOSTS\": \"['x1921c0s0b0n0', 'x1921c0s5b0n0']\",\n",
        "            \"LOCAL_RANK\": 0,\n",
        "            \"MACHINE\": \"SunSpot\",\n",
        "            \"NGPUS\": 24,\n",
        "            \"NODE_ID\": 0,\n",
        "            \"NUM_NODES\": 2,\n",
        "            \"RANK\": 0,\n",
        "            \"SCHEDULER\": \"PBS\",\n",
        "            \"WORLD_SIZE_IN_USE\": 24,\n",
        "            \"WORLD_SIZE_TOTAL\": 24\n",
        "        }\n",
        "        [2024-04-19 16:51:06][INFO][dist:602] - Using oneccl_bindings from: /lus/gila/projects/Aurora_deployment/foremans/q4-drop_sunspot/llm.devkit/torch-ccl/oneccl_bindings_for_pytorch/__init__.py\n",
        "        [2024-04-19 16:51:06][INFO][dist:604] - Using ipex from: /home/foremans/miniconda3/envs/q4-drop/lib/python3.9/site-packages/intel_extension_for_pytorch/__init__.py\n",
        "        [2024-04-19 16:51:06][INFO][dist:605] - [0/24] Using device='xpu' with backend='DDP' + 'ccl' for distributed training.\n",
        "        [2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=0/23][local_rank=0/11][node=0/1]\n",
        "        [2024-04-19 16:51:06][WARNING][dist:296] - Using [24 / 24] available \"xpu\" devices !!\n",
        "        2024:04:19-16:51:06:(16909) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n",
        "        [2024-04-19 16:51:06][INFO][test_dist:71] - model=Network(\n",
        "          (layers): Sequential(\n",
        "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
        "            (1): Linear(in_features=1024, out_features=512, bias=True)\n",
        "            (2): Linear(in_features=512, out_features=256, bias=True)\n",
        "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
        "            (4): Linear(in_features=128, out_features=128, bias=True)\n",
        "          )\n",
        "        )\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=0, loss=2709.53418, dt=1.380, dtf=0.950, dtb=0.430\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=1, loss=2058.49805, dt=0.133, dtf=0.002, dtb=0.131\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=2, loss=1507.91187, dt=0.004, dtf=0.001, dtb=0.004\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=3, loss=1181.78577, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=4, loss=949.43561, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=5, loss=848.14905, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=6, loss=788.76123, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=7, loss=753.59509, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=8, loss=750.62225, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        [2024-04-19 16:51:18][INFO][test_dist:101] - iter=9, loss=740.23474, dt=0.004, dtf=0.001, dtb=0.003\n",
        "        Application 5bf3e9e8 resources: utime=621s stime=111s maxrss=1746816KB inblock=192 oublock=16 minflt=10719359 majflt=7493 nvcsw=169332 nivcsw=77546\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "        <details closed>\n",
        "\n",
        "        <summary>\n",
        "\n",
        "        <code>CPU</code>\n",
        "\n",
        "        </summary>\n",
        "\n",
        "        ``` bash\n",
        "        $ TORCH_DEVICE=cpu mpirun -np 12 python3 test_dist.py\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=1/11][local_rank=1/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=3/11][local_rank=3/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=6/11][local_rank=6/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=5/11][local_rank=5/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=2/11][local_rank=2/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=10/11][local_rank=10/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=4/11][local_rank=4/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=7/11][local_rank=7/11][node=0/0]\n",
        "        [2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=9/11][local_rank=9/11][node=0/0]\n",
        "        [2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=11/11][local_rank=11/11][node=0/0]\n",
        "        [2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=8/11][local_rank=8/11][node=0/0]\n",
        "        [2024-04-19 14:44:13][INFO][dist:239] - DistInfo={\n",
        "            \"DEVICE\": \"cpu\",\n",
        "            \"DEVICE_ID\": \"cpu:0\",\n",
        "            \"DISTRIBUTED_BACKEND\": \"gloo\",\n",
        "            \"GPUS_PER_NODE\": 12,\n",
        "            \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/src/ezpz/hostfile\",\n",
        "            \"HOSTNAME\": \"Sams-MacBook-Pro.local\",\n",
        "            \"HOSTS\": \"['Sams-MacBook-Pro']\",\n",
        "            \"LOCAL_RANK\": 0,\n",
        "            \"MACHINE\": \"Sams-MacBook-Pro.local\",\n",
        "            \"NGPUS\": 12,\n",
        "            \"NODE_ID\": 0,\n",
        "            \"NUM_NODES\": 1,\n",
        "            \"RANK\": 0,\n",
        "            \"SCHEDULER\": \"LOCAL\",\n",
        "            \"WORLD_SIZE_IN_USE\": 12,\n",
        "            \"WORLD_SIZE_TOTAL\": 12\n",
        "        }\n",
        "        [2024-04-19 14:44:13][INFO][dist:605] - [0/12] Using device='cpu' with backend='DDP' + 'gloo' for distributed training.\n",
        "        [2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=0/11][local_rank=0/11][node=0/0]\n",
        "        [2024-04-19 14:44:13][WARNING][dist:296] - Using [12 / 12] available \"cpu\" devices !!\n",
        "        [2024-04-19 14:44:13][INFO][test_dist:72] - model=Network(\n",
        "          (layers): Sequential(\n",
        "            (0): Linear(in_features=128, out_features=1024, bias=True)\n",
        "            (1): Linear(in_features=1024, out_features=512, bias=True)\n",
        "            (2): Linear(in_features=512, out_features=256, bias=True)\n",
        "            (3): Linear(in_features=256, out_features=128, bias=True)\n",
        "            (4): Linear(in_features=128, out_features=128, bias=True)\n",
        "          )\n",
        "        )\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=0, loss=2801.62549, dt=0.389, dtf=0.042, dtb=0.348\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=1, loss=2092.84692, dt=0.051, dtf=0.010, dtb=0.041\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=2, loss=1482.45520, dt=0.037, dtf=0.004, dtb=0.033\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=3, loss=1174.38037, dt=0.033, dtf=0.002, dtb=0.031\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=4, loss=938.39917, dt=0.032, dtf=0.003, dtb=0.030\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=5, loss=888.37390, dt=0.035, dtf=0.001, dtb=0.033\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=6, loss=784.63470, dt=0.036, dtf=0.003, dtb=0.032\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=7, loss=749.53839, dt=0.033, dtf=0.002, dtb=0.031\n",
        "        [2024-04-19 14:44:14][INFO][test_dist:102] - iter=8, loss=732.22656, dt=0.036, dtf=0.003, dtb=0.034\n",
        "        [2024-04-19 14:44:15][INFO][test_dist:102] - iter=9, loss=730.63776, dt=0.034, dtf=0.001, dtb=0.033\n",
        "        35.68s user 17.20s system 546% cpu 9.681s total\n",
        "        ```\n",
        "\n",
        "        </details>\n",
        "\n",
        "        </details>\n",
        "\n",
        "## Helper Utilities\n",
        "\n",
        "We provide some shell scripts that are useful when working with a job\n",
        "scheduler (e.g. `PBS Pro` @ ALCF or `slurm` elsewhere).\n",
        "\n",
        "-   [`src/ezpz/bin/savejobenv`](./src/ezpz/bin/savejobenv): Shell script\n",
        "    to save relevant job related environment variables to a file which\n",
        "    can be `sourced` from new login instances.\n",
        "\n",
        "    <details closed>\n",
        "\n",
        "    <summary>\n",
        "\n",
        "    <b><code>savejobenv</code></b>\n",
        "\n",
        "    </summary>\n",
        "\n",
        "    Launch a job, clone (or navigate into) `ezpz`, and `source`\n",
        "    [`src/ezpz/bin/savejobenv`](./src/ezpz/bin/savejobenv):\n",
        "\n",
        "    ``` bash\n",
        "    (thetalogin4) $ qsub-gpu -A datascience -n 2 -q full-node --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" -t 06:00 -I\n",
        "    Job routed to queue \"full-node\".\n",
        "    Wait for job 10155652 to start...\n",
        "    Opening interactive session to thetagpu04\n",
        "    [...]\n",
        "    ```\n",
        "\n",
        "    ``` bash\n",
        "    (thetagpu04) $ git clone https://github.com/saforem2/ezpz\n",
        "    (thetagpu04) $ source ezpz/src/ezpz/bin/savejobenv\n",
        "    ┌───────────────────────────────────────────────────────────────────\n",
        "    │ Writing COBALT vars to /home/foremans/.cobaltenv\n",
        "    │ HOSTFILE: /var/tmp/cobalt.10155652\n",
        "    │ NHOSTS: 2\n",
        "    │ 8 GPUs per host\n",
        "    │ 16 GPUs total\n",
        "    └───────────────────────────────────────────────────────────────────\n",
        "    ┌───────────────────────────────────────────────────────────────────\n",
        "    │ [DIST INFO]:\n",
        "    │   • Writing Job info to /home/foremans/.cobaltenv\n",
        "    │     • HOSTFILE: /var/tmp/cobalt.10155652\n",
        "    │     • NHOSTS: 2\n",
        "    │     • NGPU_PER_HOST: 8\n",
        "    │     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 16\n",
        "    │ [Hosts]:\n",
        "    │       • thetagpu04 thetagpu19\n",
        "    │ [Launch]:\n",
        "    │     • Use: 'launch' (=mpirun -n  -N  --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH)\n",
        "    │       to launch job\n",
        "    └───────────────────────────────────────────────────────────────────\n",
        "    ┌────────────────────────────────────────────────────────────────────────────────\n",
        "    │ YOU ARE HERE: /home/foremans\n",
        "    │ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n",
        "    └────────────────────────────────────────────────────────────────────────────────\n",
        "    ```\n",
        "\n",
        "    </details>\n",
        "\n",
        "-   [`src/ezpz/bin/getjobenv`](./src/ezpz/bin/getjobenv): Shell script\n",
        "    that, when sourced, will populate the current environment with the\n",
        "    necessary job-related variables.\n",
        "\n",
        "    <details closed>\n",
        "\n",
        "    <summary>\n",
        "\n",
        "    <b><code>getjobenv</code></b>\n",
        "\n",
        "    </summary>\n",
        "\n",
        "    Now, in a **NEW SHELL**\n",
        "\n",
        "    ``` bash\n",
        "    (localhost)   $ ssh <user>@theta\n",
        "    ```\n",
        "\n",
        "    ``` bash\n",
        "    (thetalogin4) $ ssh thetagpu19\n",
        "    ```\n",
        "\n",
        "    ``` bash\n",
        "    (thetagpu19)  $ module load conda/2023-01-11; conda activate base\n",
        "    (thetagpu19)  $ cd ezpz\n",
        "    (thetagpu19)  $ source ./src/ezpz/bin/getjobenv\n",
        "    ┌──────────────────────────────────────────────────────────────────\n",
        "    │ [Hosts]: \n",
        "    │     • thetagpu04, thetagpu19\n",
        "    └──────────────────────────────────────────────────────────────────\n",
        "    ┌──────────────────────────────────────────────────────────────────\n",
        "    │ [DIST INFO]: \n",
        "    │     • Loading job env from: /home/foremans/.cobaltenv\n",
        "    │     • HOSTFILE: /var/tmp/cobalt.10155652\n",
        "    │     • NHOSTS: 2\n",
        "    │     • NGPU_PER_HOST: 8\n",
        "    │     • NGPUS (NHOSTS x NGPU_PER_HOST): 16\n",
        "    │     • DIST_LAUNCH: mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n",
        "    │     • Defining alias: launch: aliased to mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n",
        "    └──────────────────────────────────────────────────────────────────\n",
        "    (thetagpu19) $ mkdir -p venvs/thetaGPU/2023-01-11\n",
        "    (thetagpu19) $ python3 -m venv venvs/thetaGPU/2023-01-11 --system-site-packages\n",
        "    (thetagpu19) $ source venvs/thetaGPU/2023-01-11/bin/activate\n",
        "    (thetagpu19) $ python3 -m pip install -e . --require-virtualenv\n",
        "    (thetagpu19) $ launch python3 -m ezpz framework=pytorch backend=DDP\n",
        "    [2023-10-26 12:21:26,716][ezpz.dist][INFO] - Using DDP for distributed training\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 13\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 14\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 8\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 12\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6\n",
        "    [2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 9\n",
        "    [2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 10\n",
        "    [2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 15\n",
        "    [2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 11\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5\n",
        "    [2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2\n",
        "    [2023-10-26 12:21:26,798][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n",
        "    [2023-10-26 12:21:26,811][torch.distributed.distributed_c10d][INFO] - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,812][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,814][torch.distributed.distributed_c10d][INFO] - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,815][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,816][torch.distributed.distributed_c10d][INFO] - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,817][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,819][torch.distributed.distributed_c10d][INFO] - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,820][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,821][torch.distributed.distributed_c10d][INFO] - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,823][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,827][torch.distributed.distributed_c10d][INFO] - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,828][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,830][torch.distributed.distributed_c10d][INFO] - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:26,831][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n",
        "    [2023-10-26 12:21:27,035][ezpz.dist][INFO] - RANK: 0 / 15\n",
        "    {\n",
        "      \"framework\": \"pytorch\",\n",
        "      \"backend\": \"DDP\",\n",
        "      \"use_wandb\": false,\n",
        "      \"seed\": null,\n",
        "      \"port\": null,\n",
        "      \"ds_config_path\": null,\n",
        "      \"wandb_project_name\": null,\n",
        "      \"precision\": null,\n",
        "      \"ngpus\": null\n",
        "    }\n",
        "    [2023-10-26 12:21:27,038][__main__][INFO] - Output dir: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/ezpz/outputs/runs/pytorch/DDP/2023-10-26/12-21-25\n",
        "    [2023-10-26 12:21:27,097][ezpz.dist][INFO] - RANK: 8 / 15\n",
        "    [2023-10-26 12:21:27,103][ezpz.dist][INFO] - RANK: 6 / 15\n",
        "    [2023-10-26 12:21:27,104][ezpz.dist][INFO] - RANK: 14 / 15\n",
        "    [2023-10-26 12:21:27,111][ezpz.dist][INFO] - RANK: 13 / 15\n",
        "    [2023-10-26 12:21:27,116][ezpz.dist][INFO] - RANK: 1 / 15\n",
        "    [2023-10-26 12:21:27,126][ezpz.dist][INFO] - RANK: 7 / 15\n",
        "    [2023-10-26 12:21:27,135][ezpz.dist][INFO] - RANK: 10 / 15\n",
        "    [2023-10-26 12:21:27,139][ezpz.dist][INFO] - RANK: 12 / 15\n",
        "    [2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 9 / 15\n",
        "    [2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 15 / 15\n",
        "    [2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 11 / 15\n",
        "    [2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 5 / 15\n",
        "    [2023-10-26 12:21:27,144][ezpz.dist][INFO] - RANK: 2 / 15\n",
        "    [2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 4 / 15\n",
        "    [2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 3 / 15\n",
        "    16.56s user 30.05s system 706% cpu 6.595s total\n",
        "    ```\n",
        "\n",
        "    while this example looked at ThetaGPU, the exact same process will\n",
        "    work on any of `{ThetaGPU, Polaris, Perlmutter}`.\n",
        "\n",
        "    </details>\n",
        "\n",
        "## Look at a file\n",
        "\n",
        "``` python\n",
        "```\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "> **<span style=\"color: var(--ansi-red);\">❤️‍🩹 Status</span>**\n",
        ">\n",
        "> <pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-style: italic\">Last Updated</span>: <span style=\"color: #f06292; text-decoration-color: #f06292; font-weight: bold\">04</span><span style=\"color: #f06292; text-decoration-color: #f06292\">/</span><span style=\"color: #f06292; text-decoration-color: #f06292; font-weight: bold\">22</span><span style=\"color: #f06292; text-decoration-color: #f06292\">/</span><span style=\"color: #f06292; text-decoration-color: #f06292; font-weight: bold\">2024</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">@</span> <span style=\"color: #1a8fff; text-decoration-color: #1a8fff; font-weight: bold\">14:40:50</span>\n",
        "> </pre>\n",
        ">\n",
        "> <!-- [[![](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fsaforem2.github.io&count_bg=%2300CCFF&title_bg=%23303030&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)]{style=\"text-align:center;\"} -->\n",
        ">\n",
        "> <p align=\"center\">\n",
        ">\n",
        "> <a href=\"https://hits.seeyoufarm.com\"><img align=\"center\" src=\"https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fsaforem2.github.io%2Fezpz&count_bg=%2300CCFF&title_bg=%23303030&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false\"/></a>\n",
        ">\n",
        "> </p>\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "8014db00-5a63-4f4a-a647-65969ed7cfce"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<!--\n",
        "\n",
        "<details closed><summary><b>Deprecated:</b></summary>\n",
        "\n",
        "## Details\n",
        "\n",
        "We can `launch` on any of `{ThetaGPU, Polaris, Perlmutter}`$\\left(^{\\ast}\\right)$ \n",
        "with a specific `{framework, backend}` combo by\n",
        "\n",
        "1. [`savejobenv`](./src/ezpz/bin/savejobenv):\n",
        "\n",
        "    ```bash\n",
        "    $ source src/ezpz/bin/savejobenv\n",
        "    ```\n",
        "\n",
        "    - This will `export launch=<launcher> <launcher-opts>`\n",
        "      for `<launcher>` $\\in$ `{mpirun,mpiexec,srun}`\n",
        "      on $(^{\\ast})$ respectively.\n",
        "\n",
        "    - By default, `launch <exec>` will launch `<exec>` across\n",
        "      _all_ the available GPUs in your active `{COBALT,PBS,slurm}` job.\n",
        "\n",
        "2. `launch`\n",
        "\n",
        "    ```bash\n",
        "    $ launch $(which python3) -m ezpz framework=<framework> backend=<backend>\n",
        "    ```\n",
        "\n",
        "    - Will `launch` [`__main__.py`](./src/ezpz/__main__.py) (in this case) with framework\n",
        "    `<framework>` and backend `<backend>` (e.g. `pytorch` and `deepspeed`)\n",
        "\n",
        "\n",
        "## Frameworks\n",
        "\n",
        "### PyTorch\n",
        "\n",
        "<details closed><summary><code>DDP</code>:</summary>\n",
        "\n",
        "```bash\n",
        "launch framework=pytorch backend=DDP\n",
        "```\n",
        "\n",
        "<details closed><summary><b>Output:</b></summary>\n",
        "\n",
        "```bash\n",
        "Connected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\n",
        "Found executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\n",
        "Launching application c079ffa9-4732-45ba-995b-e5685330311b\n",
        "[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>\n",
        "\n",
        "<details closed><summary><b><code>deepspeed</code>:</b></summary>\n",
        "\n",
        "```bash\n",
        "launch framework=pytorch backend=deepspeed\n",
        "```\n",
        "\n",
        "<details closed><summary><b>Output:</b></summary>\n",
        "\n",
        "```bash\n",
        "Connected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\n",
        "Found executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\n",
        "Launching application c1c5bcd5-c300-4927-82e4-236d4643e31d\n",
        "[10/05/23 16:56:34][INFO][dist.py:362] - Using deepspeed for distributed training\n",
        "[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n",
        "[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n",
        "[2023-10-05 16:56:41,621] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 0 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 2 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 1 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 7 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 4 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 5 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 6 / 7\n",
        "[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 3 / 7\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>\n",
        "\n",
        "<details closed><summary><b><code>horovod</code></b></summary>\n",
        "\n",
        "```bash\n",
        "launch framework=pytorch backend=horovod\n",
        "```\n",
        "\n",
        "<details closed><summary><b>Output:</b></summary>\n",
        "\n",
        "```bash\n",
        "Connected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\n",
        "Found executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\n",
        "Launching application c079ffa9-4732-45ba-995b-e5685330311b\n",
        "[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n",
        "[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>\n",
        "\n",
        "### TensorFlow\n",
        "\n",
        "<details closed><summary><b><code>horovod</b></code></summary>\n",
        "\n",
        "```bash\n",
        "launch framework=tensorflow backend=horovod\n",
        "```\n",
        "\n",
        "<details closed><summary><b>Output:</b></summary>\n",
        "\n",
        "```bash\n",
        "Connected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\n",
        "Found executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\n",
        "Launching application 2b7b89f3-5f40-42de-aa12-a15876baee09\n",
        "2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:49.870940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:50.038353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:56:50.038359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
        "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
        "2023-10-05 16:57:00.277129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 4 / 7\n",
        "2023-10-05 16:57:00.303774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 0 / 7\n",
        "2023-10-05 16:57:00.430211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 5 / 7\n",
        "2023-10-05 16:57:00.445891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n",
        "2023-10-05 16:57:00.447921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 1 / 7\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 2 / 7\n",
        "2023-10-05 16:57:00.452035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 6 / 7\n",
        "2023-10-05 16:57:00.458780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 7 / 7\n",
        "2023-10-05 16:57:00.472986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -> device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n",
        "[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 3 / 7\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "</details>\n",
        "\n",
        "<!-- TESTED MACHINES\n",
        "\n",
        "### Tested Machines\n",
        "\n",
        "<details closed><summary><b>Aurora</b> (@ ALCF)</summary>\n",
        "\n",
        "```bash\n",
        "# launch job\n",
        "$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n",
        "\n",
        "# load frameworks\n",
        "$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n",
        "$ module load frameworks/.2023.12.15.001\n",
        "\n",
        "# install `ezpz`\n",
        "$ git clone https://github.com/saforem2/ezpz\n",
        "$ cd ezpz\n",
        "$ mkdir -p venvs/aurora/2023.12.15.001\n",
        "$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n",
        "$ source venvs/aurora/2023.12.15.001/bin/activate\n",
        "$ python3 -m pip install -e .\n",
        "\n",
        "# print job info and define `launch` alias\n",
        "$ source ezpz/src/ezpz/bin/savejobenv\n",
        "┌──────────────────────────────────────────────────────────────────\n",
        "│ [Hosts]:\n",
        "│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n",
        "x4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n",
        "x4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n",
        "x4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n",
        "└──────────────────────────────────────────────────────────────────\n",
        "┌──────────────────────────────────────────────────────────────────\n",
        "│ [DIST INFO]:\n",
        "│     • Loading job env from: /home/foremans/.pbsenv\n",
        "│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n",
        "│     • NHOSTS: 4\n",
        "│     • NGPU_PER_HOST: 12\n",
        "│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n",
        "│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n",
        "│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n",
        "└──────────────────────────────────────────────────────────────────\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details closed><summary><b>Polaris</b> (@ ALCF)</summary>\n",
        "\n",
        "```bash\n",
        "# Most recent `conda` versions as of 10-17-2023\n",
        "if [[ $(hostname) == x3* ]]; then\n",
        "    export MACHINE=\"polaris\"\n",
        "    export CONDA_DATE=\"2023-10-04\"\n",
        "elif [[ $(hostname) == theta* ]]; then\n",
        "    export MACHINE=\"thetaGPU\"\n",
        "    export CONDA_DATE=\"2023-01-11\"\n",
        "else\n",
        "    echo \"Unknown hostname $(hostname)\"\n",
        "fi\n",
        "module load \"conda/${CONDA_DATE}\" ; conda activate base\n",
        "# Clone saforem2/ezpz and navigate into it\n",
        "git clone https://github.com/saforem2/ezpz\n",
        "cd ezpz\n",
        "# Make a new venv for this project,\n",
        "# in the project root: ./venvs/$MACHINE/$CONDA_DATE\n",
        "VENV_DIR=\"venvs/${MACHINE}/${CONDA_DATE}\"\n",
        "python3 -m venv \"${VENV_DIR}\" --system-site-packages\n",
        "source \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\n",
        "# install `ezpz` into this `venv`\n",
        "python3 -m pip install -e .\n",
        "# to launch simple training example\n",
        "# (launches `src/ezpz/__main__.py`)\n",
        "cd src/ezpz\n",
        "./bin/train.sh framework=pytorch backend=DDP\n",
        "```\n",
        "\n",
        "</details>\n",
        "\n",
        "<details closed><summary><b>Perlmutter</b> (@ NERSC):</summary>\n",
        "\n",
        "```bash\n",
        "# request slurm allocation with `salloc`\n",
        "$ NODES=2 ; HRS=2 ; salloc --nodes $NODES --qos preempt --time $HRS:00:00 -C 'gpu&hbm80g' --gpus=$(( 4 * NODES )) -A <proj>_g\n",
        "# load `pytorch/2.0.1` module\n",
        "$ module load libfabric cudatoolkit pytorch/2.0.1\n",
        "# Clone saforem2/ezpz and navigate into it\n",
        "$ git clone https://github.com/saforem2/ezpz\n",
        "$ cd ezpz\n",
        "# update pip and install `ezpz`\n",
        "$ python3 -m pip install --upgrade pip setuptools wheel\n",
        "$ python3 -m pip install -e .\n",
        "$ cd src/ezpz\n",
        "$ ./bin/train.sh framework=pytorch backend=DDP\n",
        "```\n",
        "\n",
        "where `framework` $\\in$ `{pytorch, tensorflow}`, and `backend` $\\in$ `{DDP, deepspeed,\n",
        "horovod}`[^tf-hvd]  \n",
        "\n",
        "</details>\n",
        "\n",
        "[^tf-hvd]: Note `framework=tensorflow` is **only** compatible with `backend=horovod`\n",
        "\n",
        "- Install:\n",
        "  ```bash\n",
        "  git clone https://github.com/saforem2/ezpz\n",
        "  python3 -m pip install -e ezpz\n",
        "  ```\n",
        "\n",
        "- Determine available resources:\n",
        "  ```bash\n",
        "  [ \"$(hostname)==theta*\" ] && HOSTFILE=\"${COBALT_NODEFILE}\"  # ThetaGPU @ ALCF\n",
        "  [ \"$(hostname)==x3*\" ] && HOSTFILE=\"${PBS_NODEFILE}\"        # Polaris @ ALCF\n",
        "  [ \"$(hostname)==nid*\" ] && HOSTFILE=\"${SLURM_NODELIST}\"     # Perlmutter @ NERSC\n",
        "  NHOSTS=$(wc -l < \"${HOSTFILE}\")\n",
        "  NGPU_PER_HOST=$(nvidia-smi -L | wc -l)\n",
        "  NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\";\n",
        "  echo $NHOSTS $NGPU_PER_HOST $NGPUS\n",
        "  2 4 8\n",
        "  ```'\n",
        "\n",
        "- Example `python` script:\n",
        "\n",
        "  ```python\n",
        "  \"\"\"\n",
        "  ezpz/test.py\n",
        "  \"\"\"\n",
        "  from ezpz import setup_torch, setup_tensorflow\n",
        "\n",
        "\n",
        "  def test(\n",
        "      framework: str = 'pytorch',\n",
        "      backend: str = 'deepspeed',\n",
        "      port: str = '5432'\n",
        "  ):\n",
        "  if framework == 'pytorch':\n",
        "      _ = setup_torch(\n",
        "          backend=backend,\n",
        "          port=port,\n",
        "      )\n",
        "  elif framework == 'tensorflow':\n",
        "      _ = setup_tensorflow()\n",
        "  else:\n",
        "      raise ValueError\n",
        "\n",
        "  if __name__ == '__main__':\n",
        "      import sys\n",
        "      try:\n",
        "          framework = sys.argv[1]\n",
        "      except IndexError:\n",
        "              framework = 'pytorch'\n",
        "      try:\n",
        "          backend = sys.argv[2]\n",
        "      except IndexError:\n",
        "          backend = 'deepspeed'\n",
        "      try:\n",
        "          port = sys.argv[3]\n",
        "      except IndexError:\n",
        "          port = '5432'\n",
        "      test(framework=framework, backend=backend, port=port)\n",
        "  ```\n",
        "\n",
        "</details>\n",
        "\n",
        "-->"
      ],
      "id": "9ae8c36a-3ca4-434f-ba73-ce08f84a424b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  }
}