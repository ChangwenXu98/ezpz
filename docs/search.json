[
  {
    "objectID": "qmd/ezpz/ezpz.html",
    "href": "qmd/ezpz/ezpz.html",
    "title": "Starting Up Distributed Training",
    "section": "",
    "text": "Application Startup Time\n\n\n\n\n\n\nFrom Tanima:\n\nHi Sam and Corey,\nThanks for your comments on measuring the application start up time last week.\nTypically, we report the throughput performance after the start-up and warm-up during the “steady” state of the training.\nWe have a few follow-up questions so that we establish a methodology to address the issue brought up by Argonne.\n\nWe can set a few timestamps in the model scripts and job scripts used for the queue submission: Job script:\nTime stamp A:  \n&lt;actual python command using mpiexec&gt;\n\nInside the model script:  \nmain()  \nTimestamp B:  \n[...]\nTimestamp C:  \nFirst training steps and onwards.  \nBy startup time, do you mean measuring time difference between A and C or B and C?\n\n\n\n\nWill the measurement methodology be the same for distributed training?\nFor examples, we can measure the start-up time for the rank0?\n\n\n\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases?\nFor example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\nJob script:\nTime stamp A:\n&lt;actual python command using mpiexec&gt;\n\nTime stamp B:\n import torch\nTime stamp C\nimport IPEX\nTime stamp D\nEtc...\nIf you have any other scenarios, please feel free to suggest.\n\nThanks, Tanima.\n\n\n\n\n\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\n\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches mpiexec &lt;mpi-args&gt; python3 [...].\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n   Startup Times (Perlmutter)\n\n\n\n\n\n\n\n\nTable 1: Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#initialization-times",
    "href": "qmd/ezpz/ezpz.html#initialization-times",
    "title": "Starting Up Distributed Training",
    "section": "",
    "text": "Application Startup Time\n\n\n\n\n\n\nFrom Tanima:\n\nHi Sam and Corey,\nThanks for your comments on measuring the application start up time last week.\nTypically, we report the throughput performance after the start-up and warm-up during the “steady” state of the training.\nWe have a few follow-up questions so that we establish a methodology to address the issue brought up by Argonne.\n\nWe can set a few timestamps in the model scripts and job scripts used for the queue submission: Job script:\nTime stamp A:  \n&lt;actual python command using mpiexec&gt;\n\nInside the model script:  \nmain()  \nTimestamp B:  \n[...]\nTimestamp C:  \nFirst training steps and onwards.  \nBy startup time, do you mean measuring time difference between A and C or B and C?\n\n\n\n\nWill the measurement methodology be the same for distributed training?\nFor examples, we can measure the start-up time for the rank0?\n\n\n\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases?\nFor example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\nJob script:\nTime stamp A:\n&lt;actual python command using mpiexec&gt;\n\nTime stamp B:\n import torch\nTime stamp C\nimport IPEX\nTime stamp D\nEtc...\nIf you have any other scenarios, please feel free to suggest.\n\nThanks, Tanima.\n\n\n\n\n\n\nIn Measuring / Calculating Startup Time,I provide a summary of how the startup time is identified and calculated.\nI’m not sure exactly I understand\n\nWill the measurement methodology be the same for distributed training? For examples, we can measure the start-up time for the rank0?\n\nThe startup time is being measured for distributed training (logs only created on RANK = 0)\nI discuss in Minimal Working Example a minimal example that can be used to measure the startup times.\n\nThis is using a library I’ve been working on, ezpz that is designed to help simplify the process of setting up / initializing distributed training across many GPUs.\n\n\n\n\n\nThe startup timing was identified by parsing the logfiles from existing runs and calculating the difference \\delta t = t_{1} - t_{0},\n\nt_{0} is the time stamp at the very beginning of the shell script (defined here) which then launches mpiexec &lt;mpi-args&gt; python3 [...].\n\nt_{0} appears in the logfile as:\nJob started at: 2023-11-02-183323 on x3004c0s13b0n0\n\nt_{1} is identified as the timestamp associated with the completion of the first training step\n\nt_{1} appears in the logfile as:\n[2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n\nBelow is an example of the bash script use to parse the logfiles and identify these timestamps:\n  $ for f in $(tail -5 logfiles) ; do echo $f; cat $f | grep -E \"Job started|step=0\\,\" | uniq ; echo \"\\n\" ; done\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-183323.log\n  Job started at: 2023-11-02-183323 on x3004c0s13b0n0\n  [2023-11-02 18:34:13,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184240.log\n  Job started at: 2023-11-02-184240 on x3015c0s37b0n0\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3015c0s37b0n0-nhosts4-ngpu16-2023-11-02-184259.log\n  Job started at: 2023-11-02-184259 on x3015c0s37b0n0\n  [2023-11-02 18:43:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_SP_actCkpt_GPT125M_z0_seqlen2048_mp16_pp1_sp1_nl12_hs768_gb1_mb1/logs/foremans-x3004c0s13b0n0-nhosts4-ngpu16-2023-11-02-184407.log\n  Job started at: 2023-11-02-184407 on x3004c0s13b0n0\n  [2023-11-02 18:44:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\n\n  /lus/grand/projects/datascience/foremans/locations/polaris/projects/argonne-lcf/Megatron-DeepSpeed/outputs/gpt_actCkpt_GPT1T_4L_z1_seqlen2048_mp8_pp2_sp1_nl4_hs25600_gb16_mb2/logs/foremans-x3108c0s25b1n0-nhosts2-ngpu8-2023-11-02-192739.log\n  Job started at: 2023-11-02-192739 on x3108c0s25b1n0\n\n\n\n\n\n\n\n   Startup Times (Perlmutter)\n\n\n\n\n\n\n\n\nTable 1: Startup times on Perlmutter\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n****\nmodel_size\nworld_size\nstart\nstop\nt0\nt1\ndt\n\n\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191101.log\nGPT1T_1L\n8\n2023-10-05-191101\n2023-10-05-191215\n191101\n191215\n114\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191400.log\nGPT1T_1L\n8\n2023-10-05-191400\n2023-10-05-191511\n191400\n191511\n111\n\n\nforemans-nid008217-nhosts2-ngpu8-2023-10-05-191707.log\nGPT1T_1L\n8\n2023-10-05-191707\n2023-10-05-191817\n191707\n191817\n110\n\n\nforemans-nid008553-nhosts2-ngpu8-2023-10-15-114506.log\nGPT1T_2L\n8\n2023-10-15-114506\n2023-10-15-114616\n114506\n114616\n110\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-133531.log\nGPT2_7B\n8\n2023-10-15-133531\n2023-10-15-133745\n133531\n133745\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-135041.log\nGPT2_7B\n8\n2023-10-15-135041\n2023-10-15-135255\n135041\n135255\n214\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-140806.log\nGPT2_7B\n8\n2023-10-15-140806\n2023-10-15-141236\n140806\n141236\n430\n\n\nforemans-nid008572-nhosts2-ngpu8-2023-10-15-143120.log\nGPT2_7B\n8\n2023-10-15-143120\n2023-10-15-143655\n143120\n143655\n535\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154337.log\nGPT2_7B\n8\n2023-10-15-154337\n2023-10-15-154446\n154337\n154446\n109\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-154943.log\nGPT1T_1L\n8\n2023-10-15-154943\n2023-10-15-155317\n154943\n155317\n374\n\n\nforemans-nid008268-nhosts2-ngpu8-2023-10-15-162315.log\nGPT1T_1L\n8\n2023-10-15-162315\n2023-10-15-162441\n162315\n162441\n126\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-180714.log\nGPT2_7B\n8\n2023-10-15-180714\n2023-10-15-180805\n180714\n180805\n91\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-181733.log\nGPT2_7B\n8\n2023-10-15-181733\n2023-10-15-181834\n181733\n181834\n101\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-182228.log\nGPT1T_1L\n8\n2023-10-15-182228\n2023-10-15-183031\n182228\n183031\n803\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-183345.log\nGPT1T_2L\n8\n2023-10-15-183345\n2023-10-15-183750\n183345\n183750\n405\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-184442.log\nGPT1T_2L\n8\n2023-10-15-184442\n2023-10-15-184727\n184442\n184727\n285\n\n\nforemans-login12-nhosts2-ngpu8-2023-10-15-185952.log\nGPT1T_1L\n8\n2023-10-15-185952\n2023-10-15-190046\n185952\n190046\n4094\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-191508.log\nGPT2_7B\n8\n2023-10-15-191508\n2023-10-15-191608\n191508\n191608\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-192404.log\nGPT2_7B\n8\n2023-10-15-192404\n2023-10-15-192504\n192404\n192504\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193041.log\nGPT2_7B\n8\n2023-10-15-193041\n2023-10-15-193137\n193041\n193137\n96\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-15-193448.log\nGPT2_7B\n8\n2023-10-15-193448\n2023-10-15-193540\n193448\n193540\n92\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-195802.log\nGPT1T_1L\n16\n2023-10-15-195802\n2023-10-15-195904\n195802\n195904\n102\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200019.log\nGPT2_7B\n16\n2023-10-15-200019\n2023-10-15-200258\n200019\n200258\n239\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-200902.log\nGPT2_7B\n16\n2023-10-15-200902\n2023-10-15-201239\n200902\n201239\n337\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201524.log\nGPT2_7B\n16\n2023-10-15-201524\n2023-10-15-201612\n201524\n201612\n88\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-201834.log\nGPT2_7B\n16\n2023-10-15-201834\n2023-10-15-201923\n201834\n201923\n89\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202402.log\nGPT2_7B\n16\n2023-10-15-202402\n2023-10-15-202501\n202402\n202501\n99\n\n\nforemans-login12-nhosts4-ngpu16-2023-10-15-202606.log\nGPT2_7B\n16\n2023-10-15-202606\n2023-10-15-202713\n202606\n202713\n107\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084033.log\nGPT1T_1L\n8\n2023-10-16-084033\n2023-10-16-084212\n84033\n84212\n179\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-084628.log\nGPT1T_1L\n8\n2023-10-16-084628\n2023-10-16-084728\n84628\n84728\n100\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-085401.log\nGPT1T_1L\n8\n2023-10-16-085401\n2023-10-16-085505\n85401\n85505\n104\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-090142.log\nGPT1T_1L\n8\n2023-10-16-090142\n2023-10-16-090305\n90142\n90305\n163\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-16-093404.log\nactCkpt_GPT13B\n8\n2023-10-16-093404\n2023-10-16-093504\n93404\n93504\n100\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-101437.log\nGPT1T_1L\n16\n2023-10-16-101437\n2023-10-16-101549\n101437\n101549\n112\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-101512.log\nGPT1T_1L\n16\n2023-10-16-101512\n2023-10-16-101615\n101512\n101615\n103\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102217.log\nactCkpt_GPT25B\n16\n2023-10-16-102217\n2023-10-16-102452\n102217\n102452\n235\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-102750.log\nactCkpt_GPT25B\n16\n2023-10-16-102750\n2023-10-16-103243\n102750\n103243\n493\n\n\nforemans-nid008572-nhosts4-ngpu16-2023-10-16-103113.log\nactCkpt_GPT25B\n16\n2023-10-16-103113\n2023-10-16-103237\n103113\n103237\n124\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104037.log\nactCkpt_GPT25B\n16\n2023-10-16-104037\n2023-10-16-104148\n104037\n104148\n111\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-104819.log\nactCkpt_GPT25B\n16\n2023-10-16-104819\n2023-10-16-110002\n104819\n110002\n5183\n\n\nforemans-nid008396-nhosts4-ngpu16-2023-10-16-110119.log\nactCkpt_GPT25B\n16\n2023-10-16-110119\n2023-10-16-110225\n110119\n110225\n106\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-113715.log\nactCkpt_GPT25B\n16\n2023-10-16-113715\n2023-10-16-113824\n113715\n113824\n109\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114236.log\nGPT1T_1L\n16\n2023-10-16-114236\n2023-10-16-114338\n114236\n114338\n102\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114610.log\nGPT1T_1L\n16\n2023-10-16-114610\n2023-10-16-114711\n114610\n114711\n101\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-114819.log\nGPT1T_2L\n16\n2023-10-16-114819\n2023-10-16-114953\n114819\n114953\n134\n\n\nforemans-nid008701-nhosts4-ngpu16-2023-10-16-131058.log\nGPT1T_2L\n16\n2023-10-16-131058\n2023-10-16-131203\n131058\n131203\n145\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-151427.log\nGPT1T_1L\n4\n2023-10-16-151427\n2023-10-16-151600\n151427\n151600\n173\n\n\nforemans-nid008576-nhosts1-ngpu4-2023-10-16-152528.log\nGPT1T_1L\n4\n2023-10-16-152528\n2023-10-16-152640\n152528\n152640\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-175717.log\nGPT1T_1L\n4\n2023-10-16-175717\n2023-10-16-175829\n175717\n175829\n112\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-180457.log\nGPT1T_1L\n4\n2023-10-16-180457\n2023-10-16-180605\n180457\n180605\n148\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183116.log\nGPT1T_1L\n4\n2023-10-16-183116\n2023-10-16-183216\n183116\n183216\n100\n\n\nforemans-nid008224-nhosts1-ngpu4-2023-10-16-183921.log\nGPT1T_1L\n4\n2023-10-16-183921\n2023-10-16-184033\n183921\n184033\n112\n\n\nforemans-nid008237-nhosts1-ngpu4-2023-10-16-215614.log\nGPT1T_1L\n4\n2023-10-16-215614\n2023-10-16-215815\n215614\n215815\n201\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-052944.log\nGPT1T_1L\n4\n2023-10-17-052944\n2023-10-17-053139\n52944\n53139\n195\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053529.log\nGPT1T_1L\n4\n2023-10-17-053529\n2023-10-17-053650\n53529\n53650\n121\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-053910.log\nGPT1T_1L\n4\n2023-10-17-053910\n2023-10-17-054120\n53910\n54120\n210\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-054238.log\nGPT2_7B\n4\n2023-10-17-054238\n2023-10-17-054346\n54238\n54346\n108\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-060418.log\nGPT1T_1L\n4\n2023-10-17-060418\n2023-10-17-060600\n60418\n60600\n182\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-061514.log\nGPT1T_1L\n4\n2023-10-17-061514\n2023-10-17-061653\n61514\n61653\n139\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062102.log\nGPT1T_1L\n4\n2023-10-17-062102\n2023-10-17-062252\n62102\n62252\n150\n\n\nforemans-nid008385-nhosts1-ngpu4-2023-10-17-062445.log\nGPT1T_1L\n4\n2023-10-17-062445\n2023-10-17-062720\n62445\n62720\n275\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-064643.log\nGPT1T_1L\n8\n2023-10-17-064643\n2023-10-17-064848\n64643\n64848\n205\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-065806.log\nGPT1T_2L\n8\n2023-10-17-065806\n2023-10-17-070003\n65806\n70003\n4197\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-075152.log\nGPT1T_2L\n8\n2023-10-17-075152\n2023-10-17-075502\n75152\n75502\n350\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-080059.log\nGPT1T_2L\n8\n2023-10-17-080059\n2023-10-17-080434\n80059\n80434\n375\n\n\nforemans-nid008333-nhosts2-ngpu8-2023-10-17-081404.log\nGPT1T_2L\n8\n2023-10-17-081404\n2023-10-17-081920\n81404\n81920\n516\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-090344.log\nGPT1T_1L\n4\n2023-10-17-090344\n2023-10-17-090714\n90344\n90714\n370\n\n\nforemans-nid008228-nhosts1-ngpu4-2023-10-17-100759.log\nGPT1T_1L\n4\n2023-10-17-100759\n2023-10-17-100957\n100759\n100957\n198\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-182501.log\nGPT1T_1L\n16\n2023-10-17-182501\n2023-10-17-184001\n182501\n184001\n1500\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-193736.log\nGPT1T_1L\n16\n2023-10-17-193736\n2023-10-17-193856\n193736\n193856\n120\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-195432.log\nGPT1T_1L\n16\n2023-10-17-195432\n2023-10-17-195536\n195432\n195536\n104\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-201659.log\nGPT1T_2L\n16\n2023-10-17-201659\n2023-10-17-201823\n201659\n201823\n164\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-202949.log\nGPT1T_2L\n16\n2023-10-17-202949\n2023-10-17-203054\n202949\n203054\n105\n\n\nforemans-nid008404-nhosts4-ngpu16-2023-10-17-205848.log\nGPT1T_1L\n16\n2023-10-17-205848\n2023-10-17-205952\n205848\n205952\n104\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213244.log\nGPT1T_1L\n32\n2023-10-17-213244\n2023-10-17-213406\n213244\n213406\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-213558.log\nGPT1T_1L\n32\n2023-10-17-213558\n2023-10-17-213720\n213558\n213720\n162\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-214900.log\nGPT1T_2L\n32\n2023-10-17-214900\n2023-10-17-214959\n214900\n214959\n59\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215201.log\nGPT1T_2L\n32\n2023-10-17-215201\n2023-10-17-215309\n215201\n215309\n108\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215612.log\nGPT1T_2L\n32\n2023-10-17-215612\n2023-10-17-215726\n215612\n215726\n114\n\n\nforemans-nid008577-nhosts8-ngpu32-2023-10-17-215938.log\nGPT1T_2L\n32\n2023-10-17-215938\n2023-10-17-220044\n215938\n220044\n4106\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110001.log\nGPT1T_4L\n32\n2023-10-18-110001\n2023-10-18-110143\n110001\n110143\n142\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-110424.log\nGPT1T_8L\n32\n2023-10-18-110424\n2023-10-18-110550\n110424\n110550\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-110821.log\nGPT1T_8L\n16\n2023-10-18-110821\n2023-10-18-110952\n110821\n110952\n131\n\n\nforemans-nid008529-nhosts8-ngpu32-2023-10-18-111345.log\nGPT1T_8L\n32\n2023-10-18-111345\n2023-10-18-111458\n111345\n111458\n113\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-112531.log\nGPT1T_16L\n64\n2023-10-18-112531\n2023-10-18-112728\n112531\n112728\n197\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-113119.log\nGPT1T_16L\n64\n2023-10-18-113119\n2023-10-18-113343\n113119\n113343\n224\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113131.log\nGPT1T_4L\n16\n2023-10-18-113131\n2023-10-18-113257\n113131\n113257\n126\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-113920.log\nGPT1T_4L\n16\n2023-10-18-113920\n2023-10-18-114157\n113920\n114157\n237\n\n\nforemans-nid008197-nhosts16-ngpu64-2023-10-18-114549.log\nGPT1T_16L\n64\n2023-10-18-114549\n2023-10-18-114721\n114549\n114721\n172\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-114636.log\nGPT1T_16L\n64\n2023-10-18-114636\n2023-10-18-114805\n114636\n114805\n169\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-115808.log\nGPT1T_4L\n16\n2023-10-18-115808\n2023-10-18-120146\n115808\n120146\n4338\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-123039.log\nGPT1T_16L\n64\n2023-10-18-123039\n2023-10-18-123221\n123039\n123221\n182\n\n\nforemans-nid008389-nhosts2-ngpu8-2023-10-18-123135.log\nGPT1T_4L\n8\n2023-10-18-123135\n2023-10-18-123300\n123135\n123300\n165\n\n\nforemans-nid008244-nhosts4-ngpu16-2023-10-18-123206.log\nGPT1T_4L\n16\n2023-10-18-123206\n2023-10-18-123352\n123206\n123352\n146\n\n\nforemans-nid008456-nhosts16-ngpu64-2023-10-18-125022.log\nGPT1T_16L\n64\n2023-10-18-125022\n2023-10-18-125146\n125022\n125146\n124\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-122736.log\nGPT1T_8L\n32\n2023-10-22-122736\n2023-10-22-122844\n122736\n122844\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-123824.log\nGPT1T_8L\n32\n2023-10-22-123824\n2023-10-22-123945\n123824\n123945\n121\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-130148.log\nGPT1T_8L\n32\n2023-10-22-130148\n2023-10-22-130256\n130148\n130256\n108\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-131746.log\nGPT1T_8L\n32\n2023-10-22-131746\n2023-10-22-131909\n131746\n131909\n163\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-132700.log\nGPT1T_8L\n32\n2023-10-22-132700\n2023-10-22-132817\n132700\n132817\n117\n\n\nforemans-nid008256-nhosts8-ngpu32-2023-10-22-133459.log\nGPT1T_8L\n32\n2023-10-22-133459\n2023-10-22-133708\n133459\n133708\n249\n\n\nforemans-nid008380-nhosts4-ngpu16-2023-10-22-175049.log\nactCkpt_GPT25B\n16\n2023-10-22-175049\n2023-10-22-175230\n175049\n175230\n181\n\n\nforemans-nid008649-nhosts4-ngpu16-2023-10-22-192352.log\nGPT1T_4L\n16\n2023-10-22-192352\n2023-10-22-192530\n192352\n192530\n178\n\n\nforemans-nid008212-nhosts16-ngpu64-2023-10-23-081527.log\nGPT1T_8L\n64\n2023-10-23-081527\n2023-10-23-081702\n81527\n81702\n175\n\n\nforemans-nid008344-nhosts2-ngpu8-2023-10-23-091436.log\nGPT1T_2L\n8\n2023-10-23-091436\n2023-10-23-091610\n91436\n91610\n174\n\n\nforemans-nid008197-nhosts32-ngpu128-2023-10-24-102617.log\nGPT1T_32L\n128\n2023-10-24-102617\n2023-10-24-102826\n102617\n102826\n209\n\n\nforemans-nid008192-nhosts64-ngpu256-2023-10-24-191748.log\nGPT1T_64L\n256\n2023-10-24-191748\n2023-10-24-192021\n191748\n192021\n273\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-24-201243.log\nGPT1T_128L\n512\n2023-10-24-201243\n2023-10-24-201629\n201243\n201629\n386\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-10-26-005401.log\nGPT1T_128L\n512\n2023-10-26-005401\n2023-10-26-005811\n5401\n5811\n410\n\n\nforemans-nid008192-nhosts32-ngpu128-2023-10-26-082710.log\nGPT1T_32L\n128\n2023-10-26-082710\n2023-10-26-083049\n82710\n83049\n339\n\n\nforemans-nid008585-nhosts2-ngpu8-2023-10-31-044203.log\nGPT1T_2L\n8\n2023-10-31-044203\n2023-10-31-044533\n44203\n44533\n330\n\n\nforemans-nid008272-nhosts4-ngpu16-2023-10-31-072717.log\nGPT1T_4L\n16\n2023-10-31-072717\n2023-10-31-073131\n72717\n73131\n414\n\n\nforemans-nid008221-nhosts8-ngpu32-2023-10-31-083055.log\nGPT1T_8L\n32\n2023-10-31-083055\n2023-10-31-083545\n83055\n83545\n490\n\n\nforemans-nid008196-nhosts16-ngpu64-2023-10-31-100336.log\nGPT1T_16L\n64\n2023-10-31-100336\n2023-10-31-100848\n100336\n100848\n512\n\n\nforemans-nid008285-nhosts2-ngpu8-2023-11-01-200430.log\nGPT1T_2L\n8\n2023-11-01-200430\n2023-11-01-200829\n200430\n200829\n399\n\n\nforemans-nid008193-nhosts8-ngpu32-2023-11-01-201702.log\nGPT1T_8L\n32\n2023-11-01-201702\n2023-11-01-202131\n201702\n202131\n429\n\n\nforemans-nid008240-nhosts16-ngpu64-2023-11-01-210454.log\nGPT1T_16L\n64\n2023-11-01-210454\n2023-11-01-211007\n210454\n211007\n553\n\n\nforemans-nid008321-nhosts2-ngpu8-2023-11-02-154438.log\nGPT1T_2L\n8\n2023-11-02-154438\n2023-11-02-154949\n154438\n154949\n511\n\n\nforemans-nid008192-nhosts128-ngpu512-2023-11-04-001717.log\nGPT1T_128L\n512\n2023-11-04-001717\n2023-11-04-002124\n1717\n2124\n407"
  },
  {
    "objectID": "qmd/ezpz/ezpz.html#minimal-working-example",
    "href": "qmd/ezpz/ezpz.html#minimal-working-example",
    "title": "Starting Up Distributed Training",
    "section": "Minimal Working Example",
    "text": "Minimal Working Example\n\nAs for 3:\n\nIf we need to report the startup time for the DL applications, do we need to collect measurements using the actual Aurora NRE workloads or some small benchmarking test cases? For example, we can try to recreate the typical start-up scenarios, like library imports, and measure those separately as shown below.\n\n\nI’ve been working on a library to help simplify this:\n ezpz\nMinimal library that handles the initialization of distributed training\n\n  Working on Aurora, example:\n\nSetup / Install:\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\nLaunch with framework=pytorch, backend=DDP:\n# ----------------------------------------------------------\n# launch + startup on all workers with\n# • `framework` ∈ {`pytorch`, `tensorflow`}\n# • `backend` ∈ {`horovod`, `deepspeed`, `DDP`}\n# where `deepspeed` and `DDP` only available for `pytorch`\n# ----------------------------------------------------------\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n\n\nOutput\n\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact..."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ezpz 🍋",
    "section": "",
    "text": "ezpz 🍋\n\n\n\n\n\nLaunch and train across all your accelerators, using your favorite framework + backend combo.\nezpz simplifies the process of:\n\n\n\nSetting up + launching distributed training:\n\n\n\n\nimport ezpz as ez\n\n\nRANK = ez.setup_torch(backend=backend) for backend \\in {DDP, deepspeed, horovod}\nRANK = ez.get_rank()\nLOCAL_RANK = ez.get_local_rank()\nWORLD_SIZE = ez.get_world_size()\n\n(see ezpz/dist.py for more details).\n\n\n\n\n\n\n\nUsing your favorite {framework, backend}\n\nOn any accelerator:\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\nez.get_torch_device(): {cuda, xpu, mps, cpu}\nez.get_torch_backend(): {nccl, ccl, gloo}\n\n2ez 😎. (see frameworks for additional details)\n\n\n\n\n\nWriting device agnostic code:\n\n\n\nezpz_data_parallel.py\n\n\"\"\"\nezpz_ddp.py\n\n- to launch:\n\n  $ source ezpz/src/ezpz/bin/savejobenv\n  $ BACKEND=DDP launch python3 ezpz_ddp.py\n\"\"\"\nimport os\nimport logging\nimport torch\nimport ezpz as ez\n\n# backend can be any of DDP, deespepeed, horovod\nRANK = ez.setup_torch(\n    backend=(\n        backend := os.environ.get('BACKEND', 'DDP')\n    )\n)\nWORLD_SIZE = ez.get_world_size()\nDEVICE = ez.get_device()\n\n# log only from RANK == 0\nlogger = logging.getLogger(__name__)\nlogger.setLevel(\"INFO\") if RANK == 0 else logger.setLevel(\"CRITICAL\")\n\nmodel = torch.nn.Linear(3, 4)\nmodel.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters())\nif WORLD_SIZE &gt; 1:\n    if backend.lower() == 'ddp':\n        from torch.nn.parallel import DistributedDataParallel as DDP\n        model = DDP(model)\n    elif backend.lower() in ('ds', 'deepspeed'):\n        import deepspeed\n        model, optimizer, *_ = deepspeed.initialize(\n            model=model,\n            optimizer=optimizer\n        )\n\nx = torch.tensor([1.0, 2.0, 3.0]).to(DEVICE)\ny = model(x)\nloss = y.sum()\nif backend == 'deepspeed':\n    model.backward(loss)\n    model.step(loss)\nelse:\n    loss = loss.backward()\n    optimizer.step()\n\n\nOutput:\n\n\n\nXPU\n\n[04:50:57 PM] [foremans@x1921c0s0b0n0] ~/q/llm.devkit/Megatron-DeepSpeed/dep/ezpz/s/ezpz  main q4-drop 32s\n$ launch python3 -Wignore test_dist.py\nConnected to tcp://x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com:7919\nFound executable /home/foremans/miniconda3/envs/q4-drop/bin/python3\nLaunching application 5bf3e9e8-89fb-412a-a49e-3c81601436b7\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=9/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=14/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=3/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=17/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=6/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=13/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=7/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=19/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=8/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=21/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=10/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=22/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=11/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=23/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=2/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=20/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=4/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=15/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=18/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=12/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=1/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=16/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=5/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"ccl\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/8992337.amn-0001\",\n    \"HOSTNAME\": \"x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\",\n    \"HOSTS\": \"['x1921c0s0b0n0', 'x1921c0s5b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"SunSpot\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-04-19 16:51:06][INFO][dist:602] - Using oneccl_bindings from: /lus/gila/projects/Aurora_deployment/foremans/q4-drop_sunspot/llm.devkit/torch-ccl/oneccl_bindings_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:604] - Using ipex from: /home/foremans/miniconda3/envs/q4-drop/lib/python3.9/site-packages/intel_extension_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:605] - [0/24] Using device='xpu' with backend='DDP' + 'ccl' for distributed training.\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=0/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][WARNING][dist:296] - Using [24 / 24] available \"xpu\" devices !!\n2024:04:19-16:51:06:(16909) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16910) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16912) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16913) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16914) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16915) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16916) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16917) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16918) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16919) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16920) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16921) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n[2024-04-19 16:51:06][INFO][test_dist:71] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=0, loss=2709.53418, dt=1.380, dtf=0.950, dtb=0.430\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=1, loss=2058.49805, dt=0.133, dtf=0.002, dtb=0.131\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=2, loss=1507.91187, dt=0.004, dtf=0.001, dtb=0.004\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=3, loss=1181.78577, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=4, loss=949.43561, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=5, loss=848.14905, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=6, loss=788.76123, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=7, loss=753.59509, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=8, loss=750.62225, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=9, loss=740.23474, dt=0.004, dtf=0.001, dtb=0.003\nApplication 5bf3e9e8 resources: utime=621s stime=111s maxrss=1746816KB inblock=192 oublock=16 minflt=10719359 majflt=7493 nvcsw=169332 nivcsw=77546\n\n\n\nCPU\n\n2023-11-11 $ TORCH_DEVICE=cpu mpirun -np 12 python3 test_dist.py\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=1/11][local_rank=1/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=3/11][local_rank=3/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=6/11][local_rank=6/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=5/11][local_rank=5/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=2/11][local_rank=2/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=10/11][local_rank=10/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=4/11][local_rank=4/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=7/11][local_rank=7/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=9/11][local_rank=9/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=11/11][local_rank=11/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=8/11][local_rank=8/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"cpu\",\n    \"DEVICE_ID\": \"cpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/src/ezpz/hostfile\",\n    \"HOSTNAME\": \"Sams-MacBook-Pro.local\",\n    \"HOSTS\": \"['Sams-MacBook-Pro']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Sams-MacBook-Pro.local\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 12,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n[2024-04-19 14:44:13][INFO][dist:605] - [0/12] Using device='cpu' with backend='DDP' + 'gloo' for distributed training.\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=0/11][local_rank=0/11][node=0/0]\n[2024-04-19 14:44:13][WARNING][dist:296] - Using [12 / 12] available \"cpu\" devices !!\n[2024-04-19 14:44:13][INFO][test_dist:72] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=0, loss=2801.62549, dt=0.389, dtf=0.042, dtb=0.348\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=1, loss=2092.84692, dt=0.051, dtf=0.010, dtb=0.041\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=2, loss=1482.45520, dt=0.037, dtf=0.004, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=3, loss=1174.38037, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=4, loss=938.39917, dt=0.032, dtf=0.003, dtb=0.030\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=5, loss=888.37390, dt=0.035, dtf=0.001, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=6, loss=784.63470, dt=0.036, dtf=0.003, dtb=0.032\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=7, loss=749.53839, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=8, loss=732.22656, dt=0.036, dtf=0.003, dtb=0.034\n[2024-04-19 14:44:15][INFO][test_dist:102] - iter=9, loss=730.63776, dt=0.034, dtf=0.001, dtb=0.033\n35.68s user 17.20s system 546% cpu 9.681s total\n\n\n\n\n\nUsing wandb:\n\n\nez.setup_wandb(project_name='ezpz')\n\n\nFull support for any {device + framework + backend}:\n\ndevice: {GPU, XPU, MPS, CPU}\nframework: {torch, deepspeed, horovod, tensorflow}\nbackend: {DDP, deepspeed, horovod}"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "ezpz 🍋",
    "section": "",
    "text": "ezpz 🍋\n\n\n\n\n\nLaunch and train across all your accelerators, using your favorite framework + backend combo.\nezpz simplifies the process of:\n\n\n\nSetting up + launching distributed training:\n\n\n\n\nimport ezpz as ez\n\n\nRANK = ez.setup_torch(backend=backend) for backend \\in {DDP, deepspeed, horovod}\nRANK = ez.get_rank()\nLOCAL_RANK = ez.get_local_rank()\nWORLD_SIZE = ez.get_world_size()\n\n(see ezpz/dist.py for more details).\n\n\n\n\n\n\n\nUsing your favorite {framework, backend}\n\nOn any accelerator:\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\nez.get_torch_device(): {cuda, xpu, mps, cpu}\nez.get_torch_backend(): {nccl, ccl, gloo}\n\n2ez 😎. (see frameworks for additional details)\n\n\n\n\n\nWriting device agnostic code:\n\n\n\nezpz_data_parallel.py\n\n\"\"\"\nezpz_ddp.py\n\n- to launch:\n\n  $ source ezpz/src/ezpz/bin/savejobenv\n  $ BACKEND=DDP launch python3 ezpz_ddp.py\n\"\"\"\nimport os\nimport logging\nimport torch\nimport ezpz as ez\n\n# backend can be any of DDP, deespepeed, horovod\nRANK = ez.setup_torch(\n    backend=(\n        backend := os.environ.get('BACKEND', 'DDP')\n    )\n)\nWORLD_SIZE = ez.get_world_size()\nDEVICE = ez.get_device()\n\n# log only from RANK == 0\nlogger = logging.getLogger(__name__)\nlogger.setLevel(\"INFO\") if RANK == 0 else logger.setLevel(\"CRITICAL\")\n\nmodel = torch.nn.Linear(3, 4)\nmodel.to(DEVICE)\noptimizer = torch.optim.Adam(model.parameters())\nif WORLD_SIZE &gt; 1:\n    if backend.lower() == 'ddp':\n        from torch.nn.parallel import DistributedDataParallel as DDP\n        model = DDP(model)\n    elif backend.lower() in ('ds', 'deepspeed'):\n        import deepspeed\n        model, optimizer, *_ = deepspeed.initialize(\n            model=model,\n            optimizer=optimizer\n        )\n\nx = torch.tensor([1.0, 2.0, 3.0]).to(DEVICE)\ny = model(x)\nloss = y.sum()\nif backend == 'deepspeed':\n    model.backward(loss)\n    model.step(loss)\nelse:\n    loss = loss.backward()\n    optimizer.step()\n\n\nOutput:\n\n\n\nXPU\n\n[04:50:57 PM] [foremans@x1921c0s0b0n0] ~/q/llm.devkit/Megatron-DeepSpeed/dep/ezpz/s/ezpz  main q4-drop 32s\n$ launch python3 -Wignore test_dist.py\nConnected to tcp://x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com:7919\nFound executable /home/foremans/miniconda3/envs/q4-drop/bin/python3\nLaunching application 5bf3e9e8-89fb-412a-a49e-3c81601436b7\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=9/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=14/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=3/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=17/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=6/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=13/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=7/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=19/23][local_rank=7/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=8/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=21/23][local_rank=9/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=10/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=22/23][local_rank=10/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=11/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=23/23][local_rank=11/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=2/23][local_rank=2/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=20/23][local_rank=8/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=4/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=15/23][local_rank=3/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=18/23][local_rank=6/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=12/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=1/23][local_rank=1/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=16/23][local_rank=4/11][node=0/1]\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=5/23][local_rank=5/11][node=1/1]\n[2024-04-19 16:51:06][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"xpu\",\n    \"DEVICE_ID\": \"xpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"ccl\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/var/spool/pbs/aux/8992337.amn-0001\",\n    \"HOSTNAME\": \"x1921c0s0b0n0.hostmgmt2000.cm.americas.sgi.com\",\n    \"HOSTS\": \"['x1921c0s0b0n0', 'x1921c0s5b0n0']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"SunSpot\",\n    \"NGPUS\": 24,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 2,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"PBS\",\n    \"WORLD_SIZE_IN_USE\": 24,\n    \"WORLD_SIZE_TOTAL\": 24\n}\n[2024-04-19 16:51:06][INFO][dist:602] - Using oneccl_bindings from: /lus/gila/projects/Aurora_deployment/foremans/q4-drop_sunspot/llm.devkit/torch-ccl/oneccl_bindings_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:604] - Using ipex from: /home/foremans/miniconda3/envs/q4-drop/lib/python3.9/site-packages/intel_extension_for_pytorch/__init__.py\n[2024-04-19 16:51:06][INFO][dist:605] - [0/24] Using device='xpu' with backend='DDP' + 'ccl' for distributed training.\n[2024-04-19 16:51:06][INFO][dist:290] - [device='xpu'][rank=0/23][local_rank=0/11][node=0/1]\n[2024-04-19 16:51:06][WARNING][dist:296] - Using [24 / 24] available \"xpu\" devices !!\n2024:04:19-16:51:06:(16909) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16910) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16912) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16913) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16914) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16915) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16916) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16917) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16918) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16919) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16920) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n2024:04:19-16:51:06:(16921) |CCL_WARN| MPI was initialized externally, CCL-MPI specific environment is ignored\n[2024-04-19 16:51:06][INFO][test_dist:71] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=0, loss=2709.53418, dt=1.380, dtf=0.950, dtb=0.430\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=1, loss=2058.49805, dt=0.133, dtf=0.002, dtb=0.131\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=2, loss=1507.91187, dt=0.004, dtf=0.001, dtb=0.004\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=3, loss=1181.78577, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=4, loss=949.43561, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=5, loss=848.14905, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=6, loss=788.76123, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=7, loss=753.59509, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=8, loss=750.62225, dt=0.004, dtf=0.001, dtb=0.003\n[2024-04-19 16:51:18][INFO][test_dist:101] - iter=9, loss=740.23474, dt=0.004, dtf=0.001, dtb=0.003\nApplication 5bf3e9e8 resources: utime=621s stime=111s maxrss=1746816KB inblock=192 oublock=16 minflt=10719359 majflt=7493 nvcsw=169332 nivcsw=77546\n\n\n\nCPU\n\n2023-11-11 $ TORCH_DEVICE=cpu mpirun -np 12 python3 test_dist.py\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=1/11][local_rank=1/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=3/11][local_rank=3/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=6/11][local_rank=6/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=5/11][local_rank=5/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=2/11][local_rank=2/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=10/11][local_rank=10/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=4/11][local_rank=4/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=7/11][local_rank=7/11][node=0/0]\n[2024-04-19 14:44:12][INFO][dist:290] - [device='cpu'][rank=9/11][local_rank=9/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=11/11][local_rank=11/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=8/11][local_rank=8/11][node=0/0]\n[2024-04-19 14:44:13][INFO][dist:239] - DistInfo={\n    \"DEVICE\": \"cpu\",\n    \"DEVICE_ID\": \"cpu:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/src/ezpz/hostfile\",\n    \"HOSTNAME\": \"Sams-MacBook-Pro.local\",\n    \"HOSTS\": \"['Sams-MacBook-Pro']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"Sams-MacBook-Pro.local\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 12,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n[2024-04-19 14:44:13][INFO][dist:605] - [0/12] Using device='cpu' with backend='DDP' + 'gloo' for distributed training.\n[2024-04-19 14:44:13][INFO][dist:290] - [device='cpu'][rank=0/11][local_rank=0/11][node=0/0]\n[2024-04-19 14:44:13][WARNING][dist:296] - Using [12 / 12] available \"cpu\" devices !!\n[2024-04-19 14:44:13][INFO][test_dist:72] - model=Network(\n  (layers): Sequential(\n    (0): Linear(in_features=128, out_features=1024, bias=True)\n    (1): Linear(in_features=1024, out_features=512, bias=True)\n    (2): Linear(in_features=512, out_features=256, bias=True)\n    (3): Linear(in_features=256, out_features=128, bias=True)\n    (4): Linear(in_features=128, out_features=128, bias=True)\n  )\n)\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=0, loss=2801.62549, dt=0.389, dtf=0.042, dtb=0.348\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=1, loss=2092.84692, dt=0.051, dtf=0.010, dtb=0.041\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=2, loss=1482.45520, dt=0.037, dtf=0.004, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=3, loss=1174.38037, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=4, loss=938.39917, dt=0.032, dtf=0.003, dtb=0.030\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=5, loss=888.37390, dt=0.035, dtf=0.001, dtb=0.033\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=6, loss=784.63470, dt=0.036, dtf=0.003, dtb=0.032\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=7, loss=749.53839, dt=0.033, dtf=0.002, dtb=0.031\n[2024-04-19 14:44:14][INFO][test_dist:102] - iter=8, loss=732.22656, dt=0.036, dtf=0.003, dtb=0.034\n[2024-04-19 14:44:15][INFO][test_dist:102] - iter=9, loss=730.63776, dt=0.034, dtf=0.001, dtb=0.033\n35.68s user 17.20s system 546% cpu 9.681s total\n\n\n\n\n\nUsing wandb:\n\n\nez.setup_wandb(project_name='ezpz')\n\n\nFull support for any {device + framework + backend}:\n\ndevice: {GPU, XPU, MPS, CPU}\nframework: {torch, deepspeed, horovod, tensorflow}\nbackend: {DDP, deepspeed, horovod}"
  },
  {
    "objectID": "index.html#setup-launch",
    "href": "index.html#setup-launch",
    "title": "ezpz 🍋",
    "section": "Setup + launch",
    "text": "Setup + launch\nezpz setup on any of {thetaGPU, Polaris, Perlmutter}:\n\ngit clone + pip install:\ngit clone 'https://github.com/saforem2/ezpz'\npython3 -m pip install -e ezpz\nSave Job info + define launch alias:\n$ source ezpz/src/ezpz/bin/savejobenv\n\n\noutput\n\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0, x4415c6s6b0n0, x4415c6s7b0n0, x4415c7s0b0n0\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n\nlaunch __main__.py with pytorch + deepspeed:\n$ launch python3 -m ezpz framework=pytorch backend=deepspeed\n\n\noutput\n\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact...\n\n\n\n\nTested Machines\n\n\nAurora (@ ALCF)\n\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n\n\n\nPolaris (@ ALCF)\n\n# Most recent `conda` versions as of 10-17-2023\nif [[ $(hostname) == x3* ]]; then\n    export MACHINE=\"polaris\"\n    export CONDA_DATE=\"2023-10-04\"\nelif [[ $(hostname) == theta* ]]; then\n    export MACHINE=\"thetaGPU\"\n    export CONDA_DATE=\"2023-01-11\"\nelse\n    echo \"Unknown hostname $(hostname)\"\nfi\nmodule load \"conda/${CONDA_DATE}\" ; conda activate base\n# Clone saforem2/ezpz and navigate into it\ngit clone https://github.com/saforem2/ezpz\ncd ezpz\n# Make a new venv for this project,\n# in the project root: ./venvs/$MACHINE/$CONDA_DATE\nVENV_DIR=\"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\n# install `ezpz` into this `venv`\npython3 -m pip install -e .\n# to launch simple training example\n# (launches `src/ezpz/__main__.py`)\ncd src/ezpz\n./bin/train.sh framework=pytorch backend=DDP\n\n\n\nPerlmutter (@ NERSC):\n\n# request slurm allocation with `salloc`\n$ NODES=2 ; HRS=2 ; salloc --nodes $NODES --qos preempt --time $HRS:00:00 -C 'gpu&hbm80g' --gpus=$(( 4 * NODES )) -A &lt;proj&gt;_g\n# load `pytorch/2.0.1` module\n$ module load libfabric cudatoolkit pytorch/2.0.1\n# Clone saforem2/ezpz and navigate into it\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n# update pip and install `ezpz`\n$ python3 -m pip install --upgrade pip setuptools wheel\n$ python3 -m pip install -e .\n$ cd src/ezpz\n$ ./bin/train.sh framework=pytorch backend=DDP\nwhere framework \\in {pytorch, tensorflow}, and backend \\in {DDP, deepspeed, horovod}1"
  },
  {
    "objectID": "index.html#details",
    "href": "index.html#details",
    "title": "ezpz 🍋",
    "section": "Details",
    "text": "Details\nWe can launch on any of {ThetaGPU, Polaris, Perlmutter}\\left(^{\\ast}\\right) with a specific {framework, backend} combo by\n\nsavejobenv:\n$ source src/ezpz/bin/savejobenv\n\nThis will export launch=&lt;launcher&gt; &lt;launcher-opts&gt; for &lt;launcher&gt; \\in {mpirun,mpiexec,srun} on (^{\\ast}) respectively.\nBy default, launch &lt;exec&gt; will launch &lt;exec&gt; across all the available GPUs in your active {COBALT,PBS,slurm} job.\n\nlaunch\n$ launch $(which python3) -m ezpz framework=&lt;framework&gt; backend=&lt;backend&gt;\n\nWill launch __main__.py (in this case) with framework &lt;framework&gt; and backend &lt;backend&gt; (e.g. pytorch and deepspeed)\n\n\n\nComplete Example\n#!/bin/bash --login\ngit clone https://github.com/saforem2/ezpz\n./ezpz/src/ezpz/bin/savejobenv\nlaunch $(which python3) -m ezpz framework=&lt;framework&gt; backend=&lt;backend&gt;\nfor framework \\in {pytorch, tensorflow} and backend \\in {horovod, deepspeed, DDP}2"
  },
  {
    "objectID": "index.html#frameworks",
    "href": "index.html#frameworks",
    "title": "ezpz 🍋",
    "section": "Frameworks",
    "text": "Frameworks\n\n\nPyTorch\n\n\nDDP:\n\nlaunch framework=pytorch backend=DDP\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c079ffa9-4732-45ba-995b-e5685330311b\n[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n\n\n\n\ndeepspeed:\n\nlaunch framework=pytorch backend=deepspeed\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c1c5bcd5-c300-4927-82e4-236d4643e31d\n[10/05/23 16:56:34][INFO][dist.py:362] - Using deepspeed for distributed training\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 7 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 3 / 7\n\n\n\n\nhorovod\n\nlaunch framework=pytorch backend=horovod\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c079ffa9-4732-45ba-995b-e5685330311b\n[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n\n\n\n\n\nTensorFlow\n\n\nhorovod\n\nlaunch framework=tensorflow backend=horovod\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application 2b7b89f3-5f40-42de-aa12-a15876baee09\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:57:00.277129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 4 / 7\n2023-10-05 16:57:00.303774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 0 / 7\n2023-10-05 16:57:00.430211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 5 / 7\n2023-10-05 16:57:00.445891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n2023-10-05 16:57:00.447921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 1 / 7\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 2 / 7\n2023-10-05 16:57:00.452035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 6 / 7\n2023-10-05 16:57:00.458780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 7 / 7\n2023-10-05 16:57:00.472986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 3 / 7"
  },
  {
    "objectID": "index.html#helper-utilities",
    "href": "index.html#helper-utilities",
    "title": "ezpz 🍋",
    "section": "Helper Utilities",
    "text": "Helper Utilities\n\nsrc/ezpz/bin/savejobenv: Shell script to save relevant job related environment variables to a file which can be sourced from new login instances.\n\n\nsavejobenv\n\nLaunch a job, clone (or navigate into) ezpz, and source src/ezpz/bin/savejobenv:\n(thetalogin4) $ qsub-gpu -A datascience -n 2 -q full-node --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" -t 06:00 -I\nJob routed to queue \"full-node\".\nWait for job 10155652 to start...\nOpening interactive session to thetagpu04\n[...]\n(thetagpu04) $ git clone https://github.com/saforem2/ezpz\n(thetagpu04) $ source ezpz/src/ezpz/bin/savejobenv\n┌───────────────────────────────────────────────────────────────────\n│ Writing COBALT vars to /home/foremans/.cobaltenv\n│ HOSTFILE: /var/tmp/cobalt.10155652\n│ NHOSTS: 2\n│ 8 GPUs per host\n│ 16 GPUs total\n└───────────────────────────────────────────────────────────────────\n┌───────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│   • Writing Job info to /home/foremans/.cobaltenv\n│     • HOSTFILE: /var/tmp/cobalt.10155652\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 8\n│     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 16\n│ [Hosts]:\n│       • thetagpu04 thetagpu19\n│ [Launch]:\n│     • Use: 'launch' (=mpirun -n  -N  --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH)\n│       to launch job\n└───────────────────────────────────────────────────────────────────\n┌────────────────────────────────────────────────────────────────────────────────\n│ YOU ARE HERE: /home/foremans\n│ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n└────────────────────────────────────────────────────────────────────────────────\n\nsrc/ezpz/bin/getjobenv: Shell script that, when sourced, will populate the current environment with the necessary job-related variables.\n\n\ngetjobenv\n\nNow, in a NEW SHELL\n(localhost)   $ ssh &lt;user&gt;@theta\n(thetalogin4) $ ssh thetagpu19\n(thetagpu19)  $ module load conda/2023-01-11; conda activate base\n(thetagpu19)  $ cd ezpz\n(thetagpu19)  $ source ./src/ezpz/bin/getjobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]: \n│     • thetagpu04, thetagpu19\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]: \n│     • Loading job env from: /home/foremans/.cobaltenv\n│     • HOSTFILE: /var/tmp/cobalt.10155652\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 8\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 16\n│     • DIST_LAUNCH: mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n│     • Defining alias: launch: aliased to mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n└──────────────────────────────────────────────────────────────────\n(thetagpu19) $ mkdir -p venvs/thetaGPU/2023-01-11\n(thetagpu19) $ python3 -m venv venvs/thetaGPU/2023-01-11 --system-site-packages\n(thetagpu19) $ source venvs/thetaGPU/2023-01-11/bin/activate\n(thetagpu19) $ python3 -m pip install -e . --require-virtualenv\n(thetagpu19) $ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-10-26 12:21:26,716][ezpz.dist][INFO] - Using DDP for distributed training\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 13\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 14\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 8\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 12\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 9\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 10\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 15\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 11\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2\n[2023-10-26 12:21:26,798][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n[2023-10-26 12:21:26,811][torch.distributed.distributed_c10d][INFO] - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,812][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,814][torch.distributed.distributed_c10d][INFO] - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,815][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,816][torch.distributed.distributed_c10d][INFO] - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,817][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,819][torch.distributed.distributed_c10d][INFO] - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,820][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,821][torch.distributed.distributed_c10d][INFO] - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,823][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,827][torch.distributed.distributed_c10d][INFO] - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,828][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,830][torch.distributed.distributed_c10d][INFO] - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,831][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:27,035][ezpz.dist][INFO] - RANK: 0 / 15\n{\n  \"framework\": \"pytorch\",\n  \"backend\": \"DDP\",\n  \"use_wandb\": false,\n  \"seed\": null,\n  \"port\": null,\n  \"ds_config_path\": null,\n  \"wandb_project_name\": null,\n  \"precision\": null,\n  \"ngpus\": null\n}\n[2023-10-26 12:21:27,038][__main__][INFO] - Output dir: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/ezpz/outputs/runs/pytorch/DDP/2023-10-26/12-21-25\n[2023-10-26 12:21:27,097][ezpz.dist][INFO] - RANK: 8 / 15\n[2023-10-26 12:21:27,103][ezpz.dist][INFO] - RANK: 6 / 15\n[2023-10-26 12:21:27,104][ezpz.dist][INFO] - RANK: 14 / 15\n[2023-10-26 12:21:27,111][ezpz.dist][INFO] - RANK: 13 / 15\n[2023-10-26 12:21:27,116][ezpz.dist][INFO] - RANK: 1 / 15\n[2023-10-26 12:21:27,126][ezpz.dist][INFO] - RANK: 7 / 15\n[2023-10-26 12:21:27,135][ezpz.dist][INFO] - RANK: 10 / 15\n[2023-10-26 12:21:27,139][ezpz.dist][INFO] - RANK: 12 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 9 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 15 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 11 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 5 / 15\n[2023-10-26 12:21:27,144][ezpz.dist][INFO] - RANK: 2 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 4 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 3 / 15\n16.56s user 30.05s system 706% cpu 6.595s total\nwhile this example looked at ThetaGPU, the exact same process will work on any of {ThetaGPU, Polaris, Perlmutter}.\n\n\n\n\n\n\n\n\n\n❤️‍🩹 Status\n\n\n\n\n\n\n\nLast Updated: 04/20/2024 @ 11:35:12\n\n\n\n\n\n\n\n\n\n\n\n\n\nDeprecated:\n\n\nInstall:\ngit clone https://github.com/saforem2/ezpz\npython3 -m pip install -e ezpz\nDetermine available resources: bash [ \"$(hostname)==theta*\" ] && HOSTFILE=\"${COBALT_NODEFILE}\"  # ThetaGPU @ ALCF [ \"$(hostname)==x3*\" ] && HOSTFILE=\"${PBS_NODEFILE}\"        # Polaris @ ALCF [ \"$(hostname)==nid*\" ] && HOSTFILE=\"${SLURM_NODELIST}\"     # Perlmutter @ NERSC NHOSTS=$(wc -l &lt; \"${HOSTFILE}\") NGPU_PER_HOST=$(nvidia-smi -L | wc -l) NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"; echo $NHOSTS $NGPU_PER_HOST $NGPUS 2 4 8’\nExample python script:\n\"\"\"\nezpz/test.py\n\"\"\"\nfrom ezpz import setup_torch, setup_tensorflow\n\n\ndef test(\n    framework: str = 'pytorch',\n    backend: str = 'deepspeed',\n    port: str = '5432'\n):\nif framework == 'pytorch':\n    _ = setup_torch(\n        backend=backend,\n        port=port,\n    )\nelif framework == 'tensorflow':\n    _ = setup_tensorflow()\nelse:\n    raise ValueError\n\nif __name__ == '__main__':\n    import sys\n    try:\n        framework = sys.argv[1]\n    except IndexError:\n            framework = 'pytorch'\n    try:\n        backend = sys.argv[2]\n    except IndexError:\n        backend = 'deepspeed'\n    try:\n        port = sys.argv[3]\n    except IndexError:\n        port = '5432'\n    test(framework=framework, backend=backend, port=port)"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "ezpz 🍋",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote framework=tensorflow is only compatible with backend=horovod↩︎\ndeepspeed, DDP only support pytorch↩︎"
  },
  {
    "objectID": "qmd/details/details.html#install",
    "href": "qmd/details/details.html#install",
    "title": "✨ ezpz",
    "section": "Install",
    "text": "Install\n$ python3 -m pip install \"https://github.com/saforem2/ezpz\"\n\n!export COLORTERM=truecolor && mpirun -np 6 python3 -m ezpz framework=pytorch backend=DDP use_wandb=true\n# %%bash\n# mpirun -np 6 python3 -m ezpz framework=pytorch backend=DDP use_wandb=true\n\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=2/5][local_rank=2/11][node=0/0]\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=3/5][local_rank=3/11][node=0/0]\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=5/5][local_rank=5/11][node=0/0]\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=4/5][local_rank=4/11][node=0/0]\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=1/5][local_rank=1/11][node=0/0]\n[2024-02-06 09:53:43][INFO][dist:238] - DistInfo={\n    \"DEVICE\": \"mps\",\n    \"DEVICE_ID\": \"mps:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/qmd/details/outputs/runs/pytorch/DDP/2024-02-06/09-53-43/hostfile\",\n    \"HOSTNAME\": \"localhost\",\n    \"HOSTS\": \"['localhost']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"localhost\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 6,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n[2024-02-06 09:53:43][INFO][dist:604] - [0/6] Using device='mps' with backend='DDP' + 'gloo' for distributed training.\n[2024-02-06 09:53:43][INFO][dist:289] - [device='mps'][rank=0/5][local_rank=0/11][node=0/0]\n[2024-02-06 09:53:43][WARNING][dist:295] - Using [6 / 12] available \"mps\" devices !!\n[2024-02-06 09:53:43][INFO][dist:750] - Setting up wandb from rank: 0\n[2024-02-06 09:53:43][INFO][dist:751] - Using: WB PROJECT: ezpz\nwandb: Currently logged in as: saforem2 (l2hmc-qcd). Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.16.2\nwandb: Run data is saved locally in /Users/samforeman/projects/saforem2/ezpz/qmd/details/outputs/runs/pytorch/DDP/2024-02-06/09-53-43/wandb/run-20240206_095344-p0f6a5wc\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run cool-aardvark-238\nwandb: ⭐️ View project at https://wandb.ai/l2hmc-qcd/ezpz\nwandb: 🚀 View run at https://wandb.ai/l2hmc-qcd/ezpz/runs/p0f6a5wc\n[2024-02-06 09:53:45][INFO][dist:781] - W&B RUN: [cool-aardvark-238](https://wandb.ai/l2hmc-qcd/ezpz/runs/p0f6a5wc)\n[2024-02-06 09:53:45][INFO][dist:809] - Running on machine='localhost'\n[2024-02-06 09:53:45][INFO][dist:104] - `main` took: dt=2.1253s\nwandb: WARNING No program path found, not creating job artifact. See https://docs.wandb.ai/guides/launch/create-job\nwandb: - 0.129 MB of 0.129 MB uploadedwandb: \nwandb: Run history:\nwandb: timeit/main ▁\nwandb: \nwandb: Run summary:\nwandb: timeit/main 2.12535\nwandb: \nwandb: 🚀 View run cool-aardvark-238 at: https://wandb.ai/l2hmc-qcd/ezpz/runs/p0f6a5wc\nwandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\nwandb: Find logs at: ./wandb/run-20240206_095344-p0f6a5wc/logs\n\n\n\nfrom ezpz import get_dist_info\ndist_info = get_dist_info(verbose=True)\n\n[2024-02-06 09:53:51][INFO][dist:238] - DistInfo={\n    \"DEVICE\": \"mps\",\n    \"DEVICE_ID\": \"mps:0\",\n    \"DISTRIBUTED_BACKEND\": \"gloo\",\n    \"GPUS_PER_NODE\": 12,\n    \"HOSTFILE\": \"/Users/samforeman/projects/saforem2/ezpz/qmd/details/hostfile\",\n    \"HOSTNAME\": \"localhost\",\n    \"HOSTS\": \"['localhost']\",\n    \"LOCAL_RANK\": 0,\n    \"MACHINE\": \"localhost\",\n    \"NGPUS\": 12,\n    \"NODE_ID\": 0,\n    \"NUM_NODES\": 1,\n    \"RANK\": 0,\n    \"SCHEDULER\": \"LOCAL\",\n    \"WORLD_SIZE_IN_USE\": 1,\n    \"WORLD_SIZE_TOTAL\": 12\n}\n\n\n\n\nApple Silicon (mps) device:\n$ mpirun -np 2 python3 -c 'import ezpz; print(ezpz.get_rank()); ezpz.print_dist_setup()'\n0\n1\n[2024-01-26 07:44:19][INFO][dist:319] - [device='mps'][rank=1/1][local_rank=1/11][node=0/0]\n[2024-01-26 07:44:19][INFO][dist:319] - [device='mps'][rank=0/1][local_rank=0/11][node=0/0]\n[2024-01-26 07:44:19][WARNING][dist:325] - Using [2 / 12] available \"mps\" devices !!\n\n```` \n\n[!IMPORTANT] Scale your application across thousands of GPUs with minimal changes.\nezpz simplifies the process of setting up + launching distributed training with your favorite {framework, backend} combination:\n\nframework=pytorch + backend={DDP, deepspeed, horovod}\nframework=tensorflow + backend=horovod\n\n2ez 😎. \n\n(see frameworks for additional details)"
  },
  {
    "objectID": "qmd/details/details.html#setup-launch",
    "href": "qmd/details/details.html#setup-launch",
    "title": "✨ ezpz",
    "section": "Setup + launch",
    "text": "Setup + launch\nezpz setup on anywhere of {thetaGPU, Polaris, Perlmutter}:\n\ngit clone + pip install:\ngit clone 'https://github.com/saforem2/ezpz'\npython3 -m pip install -e ezpz\n\n\n\n\nIF YOU HAVE A SUPERCOMPUTER:\n\n\n\nSave Job info + define launch alias:\nsource ezpz/src/ezpz/bin/savejobenv\n\n\nOutput:\n\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0, x4415c6s6b0n0, x4415c6s7b0n0, x4415c7s0b0n0\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n\n\n\n\nlaunch __main__.py with pytorch + deepspeed:\n$ launch $(which python3) -m ezpz framework=pytorch backend=deepspeed\n\n\nOutput:\n\n$ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:24][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:25][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:26][INFO][dist.py:243] - Using DDP for distributed training\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:26][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:27][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:28][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:29][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:30][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:34][INFO][dist.py:292] - Using device='xpu'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][WARNING][dist.py:104] - Using backend='ccl'\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 1 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 2 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 3 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 4 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 0 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 5 / 47\n[2023-12-19 13:33:35][INFO][__main__.py:49] - {\n    \"_target_\": \"ezpz.configs.TrainConfig\",\n    \"framework\": \"pytorch\",\n    \"backend\": \"DDP\",\n    \"ds_config_path\": null,\n    \"port\": null,\n    \"seed\": null,\n    \"use_wandb\": true,\n    \"wandb_project_name\": null,\n    \"precision\": null,\n    \"ngpus\": null\n}\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 9 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 10 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 11 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 7 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 8 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 6 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 12 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 13 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 14 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 15 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 18 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 19 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 20 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 21 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 22 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 23 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 24 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 25 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 26 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 27 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 30 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 16 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 17 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 28 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 32 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 33 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 36 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 37 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 38 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 39 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 43 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 46 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 29 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 47 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 31 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 34 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 35 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 42 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 41 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 44 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 45 / 47\n[2023-12-19 13:33:35][INFO][dist.py:307] - RANK: 40 / 47\n[2023-12-19 13:33:47][INFO][dist.py:415] - Setting up wandb from rank: 0\n[2023-12-19 13:33:47][INFO][dist.py:416] - Using: WB PROJECT: ezpz\n[2023-12-19 13:33:58][INFO][dist.py:448] - W&B RUN: [flowing-wood-8](https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5)\n[2023-12-19 13:33:58][INFO][dist.py:490] - Running on x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][dist.py:506] - Reading hosts from /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n[2023-12-19 13:33:58][INFO][__main__.py:57] - Output dir: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17\n[2023-12-19 13:33:58][CRITICAL][dist.py:519] - 🚀 flowing-wood-8\n[2023-12-19 13:33:58][CRITICAL][dist.py:520] - 🔗 https://wandb.ai/l2hmc-qcd/ezpz/runs/uya29gm5\n[2023-12-19 13:33:58][CRITICAL][dist.py:521] - 📂/: /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/wandb/run-20231219_133354-uya29gm5/files\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/ezpz-pt-DDP-xpu.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/__main__.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-17/main_debug.log to W&B artifact...\n[2023-12-19 13:33:58][INFO][dist.py:563] - Adding /lus/gecko/projects/Aurora_deployment/foremans/projects/saforem2/ezpz/src/ezpz/outputs/runs/pytorch/DDP/2023-12-19/13-33-16/__main__.log to W&B artifact...\n\n\n\n\n\nTested Machines\n\n\n\n\n\nAurora (@ ALCF)\n\n\n\n# launch job\n$ qsub -q EarlyAppAccess -A Aurora_Deployment -l walltime=2:00:00 -l select=4 -I\n\n# load frameworks\n$ module use -a /soft/modulefiles ; module --ignore_cache load frameworks\n$ module load frameworks/.2023.12.15.001\n\n# install `ezpz`\n$ git clone https://github.com/saforem2/ezpz\n$ cd ezpz\n$ mkdir -p venvs/aurora/2023.12.15.001\n$ python3 -m venv venvs/aurora/2023.12.15.001 --system-site-packages\n$ source venvs/aurora/2023.12.15.001/bin/activate\n$ python3 -m pip install -e .\n\n# print job info and define `launch` alias\n$ source ezpz/src/ezpz/bin/savejobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]:\n│     • x4415c6s5b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s6b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c6s7b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\nx4415c7s0b0n0.hostmgmt2415.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│     • Loading job env from: /home/foremans/.pbsenv\n│     • HOSTFILE: /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • NHOSTS: 4\n│     • NGPU_PER_HOST: 12\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 48\n│     • DIST_LAUNCH: mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n│     • Defining alias: launch: aliased to mpiexec --verbose --envall -n 48 -ppn 12 --hostfile /var/spool/pbs/aux/297306.aurora-pbs-0001.hostmgmt.cm.aurora.alcf.anl.gov\n└──────────────────────────────────────────────────────────────────\n\n\n\n\n\nPolaris (@ ALCF)\n\n\n# Most recent `conda` versions as of 10-17-2023\nif [[ $(hostname) == x3* ]]; then\n    export MACHINE=\"polaris\"\n    export CONDA_DATE=\"2023-10-04\"\nelif [[ $(hostname) == theta* ]]; then\n    export MACHINE=\"thetaGPU\"\n    export CONDA_DATE=\"2023-01-11\"\nelse\n    echo \"Unknown hostname $(hostname)\"\nfi\nmodule load \"conda/${CONDA_DATE}\" ; conda activate base\n# Clone saforem2/ezpz and navigate into it\ngit clone https://github.com/saforem2/ezpz\ncd ezpz\n# Make a new venv for this project,\n# in the project root: ./venvs/$MACHINE/$CONDA_DATE\nVENV_DIR=\"venvs/${MACHINE}/${CONDA_DATE}\"\npython3 -m venv \"${VENV_DIR}\" --system-site-packages\nsource \"venvs/${MACHINE}/${CONDA_DATE}/bin/activate\"\n# install `ezpz` into this `venv`\npython3 -m pip install -e .\n# to launch simple training example\n# (launches `src/ezpz/__main__.py`)\ncd src/ezpz\n./bin/train.sh framework=pytorch backend=DDP\n\n\n\n\nPerlmutter (@ NERSC):\n\n\n# request slurm allocation with `salloc`\nNODES=2 ; HRS=2 ; salloc --nodes $NODES --qos preempt --time $HRS:00:00 -C 'gpu&hbm80g' --gpus=$(( 4 * NODES )) -A &lt;proj&gt;_g\n# load `pytorch/2.0.1` module\nmodule load libfabric cudatoolkit pytorch/2.0.1\n# Clone saforem2/ezpz and navigate into it\ngit clone https://github.com/saforem2/ezpz\ncd ezpz\n# update pip and install `ezpz`\npython3 -m pip install --upgrade pip setuptools wheel\npython3 -m pip install -e .\ncd src/ezpz\n./bin/train.sh framework=pytorch backend=DDP\nwhere framework \\in {pytorch, tensorflow}, and backend \\in {DDP, deepspeed, horovod}1\n\n\n\nDeprecated:\n\n\nInstall:\ngit clone https://github.com/saforem2/ezpz\npython3 -m pip install -e ezpz\nDetermine available resources: bash [ \"$(hostname)==theta*\" ] && HOSTFILE=\"${COBALT_NODEFILE}\"  # ThetaGPU @ ALCF [ \"$(hostname)==x3*\" ] && HOSTFILE=\"${PBS_NODEFILE}\"        # Polaris @ ALCF [ \"$(hostname)==nid*\" ] && HOSTFILE=\"${SLURM_NODELIST}\"     # Perlmutter @ NERSC NHOSTS=$(wc -l &lt; \"${HOSTFILE}\") NGPU_PER_HOST=$(nvidia-smi -L | wc -l) NGPUS=\"$((${NHOSTS}*${NGPU_PER_HOST}))\"; echo $NHOSTS $NGPU_PER_HOST $NGPUS 2 4 8’\nExample python script:\n\"\"\"\nezpz/test.py\n\"\"\"\nfrom ezpz import setup_torch, setup_tensorflow\n\n\ndef test(\n    framework: str = 'pytorch',\n    backend: str = 'deepspeed',\n    port: str = '5432'\n):\nif framework == 'pytorch':\n    _ = setup_torch(\n        backend=backend,\n        port=port,\n    )\nelif framework == 'tensorflow':\n    _ = setup_tensorflow()\nelse:\n    raise ValueError\n\nif __name__ == '__main__':\n    import sys\n    try:\n        framework = sys.argv[1]\n    except IndexError:\n            framework = 'pytorch'\n    try:\n        backend = sys.argv[2]\n    except IndexError:\n        backend = 'deepspeed'\n    try:\n        port = sys.argv[3]\n    except IndexError:\n        port = '5432'\n    test(framework=framework, backend=backend, port=port)"
  },
  {
    "objectID": "qmd/details/details.html#details",
    "href": "qmd/details/details.html#details",
    "title": "✨ ezpz",
    "section": "Details",
    "text": "Details\n\n[!NOTE] We can launch on any of {ThetaGPU, Polaris, Perlmutter}\\left(^{\\ast}\\right) with a specific {framework, backend} combo by 1. savejobenv: - This will export launch=&lt;launcher&gt; &lt;launcher-opts&gt; for &lt;launcher&gt; \\in {mpirun,mpiexec,srun} on (^{\\ast}) respectively. - By default, launch &lt;exec&gt; will launch &lt;exec&gt; across all the available GPUs in your active {COBALT,PBS,slurm} job. 2. launch - e.g. launch $(which python3) -m ezpz framework=&lt;framework&gt; backend=&lt;backend&gt;, will: - launch __main__.py (in this case) with framework &lt;framework&gt; and backend &lt;backend&gt; (e.g. pytorch and deepspeed)\n\n\nComplete Example\n#!/bin/bash --login\ngit clone https://github.com/saforem2/ezpz\n./ezpz/src/ezpz/bin/savejobenv\nlaunch $(which python3) -m ezpz framework=&lt;framework&gt; backend=&lt;backend&gt;\nfor framework \\in {pytorch, tensorflow} and backend \\in {horovod, deepspeed, DDP}2"
  },
  {
    "objectID": "qmd/details/details.html#frameworks",
    "href": "qmd/details/details.html#frameworks",
    "title": "✨ ezpz",
    "section": "Frameworks",
    "text": "Frameworks\n\n\n\nPyTorch\n\n\n\n\n\nDDP:\n\n\nlaunch framework=pytorch backend=DDP\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c079ffa9-4732-45ba-995b-e5685330311b\n[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n\n\n\n\n\ndeepspeed:\n\n\nlaunch framework=pytorch backend=deepspeed\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c1c5bcd5-c300-4927-82e4-236d4643e31d\n[10/05/23 16:56:34][INFO][dist.py:362] - Using deepspeed for distributed training\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,949] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:34,953] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,160] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,160] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:40,767] [INFO] [comm.py:637:init_distributed] cdb=None\n[2023-10-05 16:56:40,767] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=4, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=5, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=6, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=1, local_rank=1, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=7, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=2, local_rank=2, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=3, local_rank=3, world_size=8, master_addr=10.140.57.89, master_port=29500\n[2023-10-05 16:56:41,621] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 7 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:41][INFO][dist.py:413] - RANK: 3 / 7\n\n\n\n\n\nhorovod\n\n\nlaunch framework=pytorch backend=horovod\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application c079ffa9-4732-45ba-995b-e5685330311b\n[10/05/23 16:56:26][INFO][dist.py:362] - Using DDP for distributed training\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 0 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 2 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 4 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 3 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 1 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 6 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 5 / 7\n[10/05/23 16:56:27][INFO][dist.py:413] - RANK: 7 / 7\n\n\n\n\n\n\nTensorFlow\n\n\nlaunch framework=tensorflow backend=horovod\n\n\nOutput:\n\nConnected to tcp://x3005c0s31b1n0.hsn.cm.polaris.alcf.anl.gov:7919\nFound executable /soft/datascience/conda/2023-10-04/mconda3/bin/python3\nLaunching application 2b7b89f3-5f40-42de-aa12-a15876baee09\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:49.870940: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038355: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038353: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:56:50.038359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-10-05 16:57:00.277129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 4 / 7\n2023-10-05 16:57:00.303774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:07:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 0 / 7\n2023-10-05 16:57:00.430211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 5 / 7\n2023-10-05 16:57:00.445891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 1, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:46:00.0,compute capability: 8.0\n2023-10-05 16:57:00.447921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 1 / 7\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 2 / 7\n2023-10-05 16:57:00.452035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 2, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:85:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 6 / 7\n2023-10-05 16:57:00.458780: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 7 / 7\n2023-10-05 16:57:00.472986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38341 MB memory:  -&gt; device: 3, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:c7:00.0,compute capability: 8.0\n[10/05/23 16:57:00][INFO][dist.py:203] - RANK: 3 / 7"
  },
  {
    "objectID": "qmd/details/details.html#helper-utilities",
    "href": "qmd/details/details.html#helper-utilities",
    "title": "✨ ezpz",
    "section": "Helper Utilities",
    "text": "Helper Utilities\n\nsrc/ezpz/bin/savejobenv: Shell script to save relevant job related environment variables to a file which can be sourced from new login instances.\nsrc/ezpz/bin/getjobenv: Shell script that, when sourced, will populate the current environment with the necessary job-related variables.\n\n\n\n\nsavejobenv\n\n\nLaunch a job, clone (or navigate into) ezpz, and source src/ezpz/bin/savejobenv:\n(thetalogin4) $ qsub-gpu -A datascience -n 2 -q full-node --attrs=\"filesystems=home,grand,eagle,theta-fs0:ssds=required\" -t 06:00 -I\nJob routed to queue \"full-node\".\nWait for job 10155652 to start...\nOpening interactive session to thetagpu04\n[...]\n(thetagpu04) $ git clone https://github.com/saforem2/ezpz\n(thetagpu04) $ source ezpz/src/ezpz/bin/savejobenv\n┌───────────────────────────────────────────────────────────────────\n│ Writing COBALT vars to /home/foremans/.cobaltenv\n│ HOSTFILE: /var/tmp/cobalt.10155652\n│ NHOSTS: 2\n│ 8 GPUs per host\n│ 16 GPUs total\n└───────────────────────────────────────────────────────────────────\n┌───────────────────────────────────────────────────────────────────\n│ [DIST INFO]:\n│   • Writing Job info to /home/foremans/.cobaltenv\n│     • HOSTFILE: /var/tmp/cobalt.10155652\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 8\n│     • NGPUS = (NHOSTS * NGPU_PER_HOST) = 16\n│ [Hosts]:\n│       • thetagpu04 thetagpu19\n│ [Launch]:\n│     • Use: 'launch' (=mpirun -n  -N  --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH)\n│       to launch job\n└───────────────────────────────────────────────────────────────────\n┌────────────────────────────────────────────────────────────────────────────────\n│ YOU ARE HERE: /home/foremans\n│ Run 'source ./bin/getjobenv' in a NEW SHELL to automatically set env vars\n└────────────────────────────────────────────────────────────────────────────────\n\n\n\n\ngetjobenv\n\n\nNow, in a NEW SHELL\n(localhost)   $ ssh &lt;user&gt;@theta\n(thetalogin4) $ ssh thetagpu19\n(thetagpu19)  $ module load conda/2023-01-11; conda activate base\n(thetagpu19)  $ cd ezpz\n(thetagpu19)  $ source ./src/ezpz/bin/getjobenv\n┌──────────────────────────────────────────────────────────────────\n│ [Hosts]: \n│     • thetagpu04, thetagpu19\n└──────────────────────────────────────────────────────────────────\n┌──────────────────────────────────────────────────────────────────\n│ [DIST INFO]: \n│     • Loading job env from: /home/foremans/.cobaltenv\n│     • HOSTFILE: /var/tmp/cobalt.10155652\n│     • NHOSTS: 2\n│     • NGPU_PER_HOST: 8\n│     • NGPUS (NHOSTS x NGPU_PER_HOST): 16\n│     • DIST_LAUNCH: mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n│     • Defining alias: launch: aliased to mpirun -n 16 -N 8 --hostfile /var/tmp/cobalt.10155652 -x PATH -x LD_LIBRARY_PATH\n└──────────────────────────────────────────────────────────────────\n(thetagpu19) $ mkdir -p venvs/thetaGPU/2023-01-11\n(thetagpu19) $ python3 -m venv venvs/thetaGPU/2023-01-11 --system-site-packages\n(thetagpu19) $ source venvs/thetaGPU/2023-01-11/bin/activate\n(thetagpu19) $ python3 -m pip install -e . --require-virtualenv\n(thetagpu19) $ launch python3 -m ezpz framework=pytorch backend=DDP\n[2023-10-26 12:21:26,716][ezpz.dist][INFO] - Using DDP for distributed training\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 13\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 14\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 8\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 12\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 6\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 9\n[2023-10-26 12:21:26,787][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 10\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 15\n[2023-10-26 12:21:26,788][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 11\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 7\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 3\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 1\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 4\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 5\n[2023-10-26 12:21:26,789][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 2\n[2023-10-26 12:21:26,798][torch.distributed.distributed_c10d][INFO] - Added key: store_based_barrier_key:1 to store for rank: 0\n[2023-10-26 12:21:26,811][torch.distributed.distributed_c10d][INFO] - Rank 14: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,812][torch.distributed.distributed_c10d][INFO] - Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,814][torch.distributed.distributed_c10d][INFO] - Rank 13: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,815][torch.distributed.distributed_c10d][INFO] - Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,816][torch.distributed.distributed_c10d][INFO] - Rank 8: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,817][torch.distributed.distributed_c10d][INFO] - Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,819][torch.distributed.distributed_c10d][INFO] - Rank 12: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,820][torch.distributed.distributed_c10d][INFO] - Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,821][torch.distributed.distributed_c10d][INFO] - Rank 10: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,823][torch.distributed.distributed_c10d][INFO] - Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 9: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,825][torch.distributed.distributed_c10d][INFO] - Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,827][torch.distributed.distributed_c10d][INFO] - Rank 15: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,828][torch.distributed.distributed_c10d][INFO] - Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,830][torch.distributed.distributed_c10d][INFO] - Rank 11: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:26,831][torch.distributed.distributed_c10d][INFO] - Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 16 nodes.\n[2023-10-26 12:21:27,035][ezpz.dist][INFO] - RANK: 0 / 15\n{\n  \"framework\": \"pytorch\",\n  \"backend\": \"DDP\",\n  \"use_wandb\": false,\n  \"seed\": null,\n  \"port\": null,\n  \"ds_config_path\": null,\n  \"wandb_project_name\": null,\n  \"precision\": null,\n  \"ngpus\": null\n}\n[2023-10-26 12:21:27,038][__main__][INFO] - Output dir: /lus/grand/projects/datascience/foremans/locations/thetaGPU/projects/saforem2/ezpz/outputs/runs/pytorch/DDP/2023-10-26/12-21-25\n[2023-10-26 12:21:27,097][ezpz.dist][INFO] - RANK: 8 / 15\n[2023-10-26 12:21:27,103][ezpz.dist][INFO] - RANK: 6 / 15\n[2023-10-26 12:21:27,104][ezpz.dist][INFO] - RANK: 14 / 15\n[2023-10-26 12:21:27,111][ezpz.dist][INFO] - RANK: 13 / 15\n[2023-10-26 12:21:27,116][ezpz.dist][INFO] - RANK: 1 / 15\n[2023-10-26 12:21:27,126][ezpz.dist][INFO] - RANK: 7 / 15\n[2023-10-26 12:21:27,135][ezpz.dist][INFO] - RANK: 10 / 15\n[2023-10-26 12:21:27,139][ezpz.dist][INFO] - RANK: 12 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 9 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 15 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 11 / 15\n[2023-10-26 12:21:27,141][ezpz.dist][INFO] - RANK: 5 / 15\n[2023-10-26 12:21:27,144][ezpz.dist][INFO] - RANK: 2 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 4 / 15\n[2023-10-26 12:21:27,145][ezpz.dist][INFO] - RANK: 3 / 15\n16.56s user 30.05s system 706% cpu 6.595s total\nwhile this example looked at ThetaGPU, the exact same process will work on any of {ThetaGPU, Polaris, Perlmutter}."
  },
  {
    "objectID": "qmd/details/details.html#footnotes",
    "href": "qmd/details/details.html#footnotes",
    "title": "✨ ezpz",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote framework=tensorflow is only compatible with backend=horovod↩︎\ndeepspeed, DDP only support pytorch↩︎"
  }
]